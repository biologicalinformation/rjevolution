<!doctype html><html lang=it-it><head><script async src="https://www.googletagmanager.com/gtag/js?id=G-H41E1B9G7L"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-H41E1B9G7L",{anonymize_ip:!1})}</script><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><title>Verso una teoria dell'evoluzione come apprendimento multilivello | Appunti di Etica ed Evoluzione del Ringiovanimento</title><meta name=viewport content="width=device-width,minimum-scale=1"><meta name=description content="Toward a theory of evolution as multilevel learning
Vitaly Vanchurin, Yuri I. Wolf, Mikhail I. Katsnelson, and Eugene V. Koonin
Contributed by Eugene V. Koonin; received November 2, 2021; accepted January 3, 2022; reviewed by Steven Frank and Eörs Szathmáry
February 4, 2022
https://doi.org/10.1073/pnas.2120037119
Significato La moderna teoria evolutiva fornisce una descrizione quantitativa dettagliata dei processi microevolutivi che si verificano all&rsquo;interno di popolazioni di organismi in evoluzione, ma le transizioni evolutive e l&rsquo;emergere di molteplici livelli di complessità rimangono poco comprese."><meta name=generator content="Hugo 0.107.0"><meta name=robots content="noindex, nofollow"><link rel=stylesheet href=/rjevolution/ananke/css/main.min.css><meta property="og:title" content="Verso una teoria dell'evoluzione come apprendimento multilivello"><meta property="og:description" content="Toward a theory of evolution as multilevel learning
Vitaly Vanchurin, Yuri I. Wolf, Mikhail I. Katsnelson, and Eugene V. Koonin
Contributed by Eugene V. Koonin; received November 2, 2021; accepted January 3, 2022; reviewed by Steven Frank and Eörs Szathmáry
February 4, 2022
https://doi.org/10.1073/pnas.2120037119
Significato La moderna teoria evolutiva fornisce una descrizione quantitativa dettagliata dei processi microevolutivi che si verificano all&rsquo;interno di popolazioni di organismi in evoluzione, ma le transizioni evolutive e l&rsquo;emergere di molteplici livelli di complessità rimangono poco comprese."><meta property="og:type" content="article"><meta property="og:url" content="https://biologicinfo.github.io/rjevolution/papers/toward-a-theory-of-evolution-as-multilevel-learning/"><meta property="article:section" content="papers"><meta property="article:published_time" content="2022-02-04T09:03:23+02:00"><meta property="article:modified_time" content="2022-02-04T09:03:23+02:00"><meta itemprop=name content="Verso una teoria dell'evoluzione come apprendimento multilivello"><meta itemprop=description content="Toward a theory of evolution as multilevel learning
Vitaly Vanchurin, Yuri I. Wolf, Mikhail I. Katsnelson, and Eugene V. Koonin
Contributed by Eugene V. Koonin; received November 2, 2021; accepted January 3, 2022; reviewed by Steven Frank and Eörs Szathmáry
February 4, 2022
https://doi.org/10.1073/pnas.2120037119
Significato La moderna teoria evolutiva fornisce una descrizione quantitativa dettagliata dei processi microevolutivi che si verificano all&rsquo;interno di popolazioni di organismi in evoluzione, ma le transizioni evolutive e l&rsquo;emergere di molteplici livelli di complessità rimangono poco comprese."><meta itemprop=datePublished content="2022-02-04T09:03:23+02:00"><meta itemprop=dateModified content="2022-02-04T09:03:23+02:00"><meta itemprop=wordCount content="12195"><meta itemprop=keywords content><meta name=twitter:card content="summary"><meta name=twitter:title content="Verso una teoria dell'evoluzione come apprendimento multilivello"><meta name=twitter:description content="Toward a theory of evolution as multilevel learning
Vitaly Vanchurin, Yuri I. Wolf, Mikhail I. Katsnelson, and Eugene V. Koonin
Contributed by Eugene V. Koonin; received November 2, 2021; accepted January 3, 2022; reviewed by Steven Frank and Eörs Szathmáry
February 4, 2022
https://doi.org/10.1073/pnas.2120037119
Significato La moderna teoria evolutiva fornisce una descrizione quantitativa dettagliata dei processi microevolutivi che si verificano all&rsquo;interno di popolazioni di organismi in evoluzione, ma le transizioni evolutive e l&rsquo;emergere di molteplici livelli di complessità rimangono poco comprese."></head><body class="ma0 avenir bg-near-white"><header><div class=bg-blue><nav class="pv3 ph3 ph4-ns" role=navigation><div class="flex-l justify-between items-center center"><a href=/rjevolution/ class="f3 fw2 hover-white no-underline white-90 dib">Appunti di Etica ed Evoluzione del Ringiovanimento</a><div class="flex-l items-center"><ul class="pl0 mr3"><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white no-underline white-90" href=/rjevolution/pages/about title="About pagina">About</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white no-underline white-90" href=/rjevolution/news title="News pagina">News</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white no-underline white-90" href=/rjevolution/pages title="Pagine pagina">Pagine</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white no-underline white-90" href=/rjevolution/papers title="Papers pagina">Papers</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white no-underline white-90" href=/rjevolution/posts title="Posts pagina">Posts</a></li></ul><div class=ananke-socials></div></div></div></nav></div></header><main class=pb7 role=main><article class="flex-l flex-wrap justify-between mw8 center ph3"><header class="mt4 w-100"><aside class="instapaper_ignoref b helvetica tracked">PAPERS</aside><div id=sharing class="mt3 ananke-socials"></div><h1 class="f1 athelas mt3 mb1">Verso una teoria dell'evoluzione come apprendimento multilivello</h1><time class="f6 mv4 dib tracked" datetime=2022-02-04T09:03:23+02:00>febbraio 4, 2022</time></header><div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><p><strong>Toward a theory of evolution as multilevel learning</strong></p><p>Vitaly Vanchurin, Yuri I. Wolf, Mikhail I. Katsnelson, and Eugene V. Koonin</p><p>Contributed by Eugene V. Koonin; received November 2, 2021; accepted January 3, 2022; reviewed by Steven Frank and Eörs Szathmáry</p><p>February 4, 2022</p><p><a href=https://doi.org/10.1073/pnas.2120037119>https://doi.org/10.1073/pnas.2120037119</a></p><h2 id=significato>Significato</h2><p>La moderna teoria evolutiva fornisce una descrizione quantitativa dettagliata dei processi microevolutivi che si verificano all&rsquo;interno di popolazioni di organismi in evoluzione, ma le transizioni evolutive e l&rsquo;emergere di molteplici livelli di complessità rimangono poco comprese. In questa sede, stabiliamo la corrispondenza tra le caratteristiche chiave dell&rsquo;evoluzione, della dinamica dell&rsquo;apprendimento e della rinormalizzabilità delle teorie fisiche per delineare una teoria dell&rsquo;evoluzione che cerca di incorporare tutti i processi evolutivi in un quadro matematico unificato della teoria dell&rsquo;apprendimento. Secondo questa teoria, ad esempio, la replicazione del materiale genetico e la selezione naturale emergono facilmente dalle dinamiche di apprendimento e, in sistemi sufficientemente complessi, gli stessi fenomeni di apprendimento si verificano su più livelli o su scale diverse, analogamente al caso delle teorie fisiche rinormalizzabili.</p><h2 id=abstract>Abstract</h2><p>Applichiamo la teoria dell&rsquo;apprendimento ai sistemi fisicamente rinormalizzabili nel tentativo di delineare una teoria dell&rsquo;evoluzione biologica, compresa l&rsquo;origine della vita, come apprendimento multilivello. Formuliamo sette principi fondamentali dell&rsquo;evoluzione che sembrano essere necessari e sufficienti per rendere osservabile un universo e mostriamo che essi comportano le principali caratteristiche dell&rsquo;evoluzione biologica, tra cui la replicazione e la selezione naturale. Si dimostra che questi fenomeni cardine della biologia emergono dalle caratteristiche fondamentali delle dinamiche di apprendimento, come l&rsquo;esistenza di una funzione di perdita, che viene minimizzata durante l&rsquo;apprendimento. In seguito abbozziamo la teoria dell&rsquo;evoluzione utilizzando il quadro matematico delle reti neurali, che consente un&rsquo;analisi dettagliata dei fenomeni evolutivi. Per dimostrare il potenziale del quadro teorico proposto, ricaviamo una versione generalizzata del dogma centrale della biologia molecolare analizzando il flusso di informazioni durante l&rsquo;apprendimento (back propagation) e la previsione (forward propagation) dell&rsquo;ambiente da parte degli organismi in evoluzione. I fenomeni evolutivi più complessi, come le grandi transizioni nell&rsquo;evoluzione (in particolare, l&rsquo;origine della vita), devono essere analizzati nel limite termodinamico, che è descritto in dettaglio nell&rsquo;articolo di Vanchurin et al. [V. Vanchurin, Y. I. Wolf, E. V. Koonin, M. I. Katsnelson, Proc. Natl. Acad. Sci. U.S.A. 119, 10.1073/pnas.2120042119 (2022)].</p><hr><p>Che cos&rsquo;è la vita? Se questa domanda viene posta in ambito scientifico piuttosto che filosofico, una risposta soddisfacente dovrebbe assumere la forma di un modello teorico dell&rsquo;origine e dell&rsquo;evoluzione dei sistemi complessi che si identificano con la vita (1). La NASA ha definito operativamente la vita come segue: &ldquo;La vita è un sistema chimico autosufficiente capace di evoluzione darwiniana&rdquo; (2, 3). A parte l&rsquo;insistenza sulla chimica, l&rsquo;evoluzione a lungo termine che comporta mutazioni (casuali), diversificazione e adattamento è, in effetti, una caratteristica intrinseca ed essenziale della vita che non è evidente in nessun altro fenomeno naturale.
Il problema di questa definizione, tuttavia, è che la stessa selezione naturale (darwiniana) sembra essere un fenomeno complesso piuttosto che elementare (4). In tutti gli organismi in evoluzione di cui siamo a conoscenza, affinché la selezione naturale si avvii e sostenga l&rsquo;evoluzione a lungo termine, una condizione essenziale è la replicazione di un complesso vettore di informazioni digitali (una molecola di DNA o RNA). La fedeltà di replicazione deve essere sufficientemente alta da consentire la replicazione differenziale dei mutanti emergenti e la sopravvivenza di quelli più adatti (questo livello di fedeltà di replicazione è spesso indicato come soglia di Eigen) (5). Negli organismi moderni, l&rsquo;accuratezza della replicazione è garantita da elaborate macchine molecolari che comprendono non solo gli enzimi di replicazione e riparazione, ma anche l&rsquo;intera rete metabolica della cellula, che fornisce l&rsquo;energia e i mattoni per la replicazione. L&rsquo;origine della vita è quindi un tipico problema di &ldquo;uovo e gallina&rdquo; (o &ldquo;catch-22&rdquo;): una replicazione accurata è essenziale per l&rsquo;evoluzione, ma i meccanismi che garantiscono la fedeltà della replicazione sono essi stessi prodotti di complessi processi evolutivi (6, 7).</p><p>Poiché la replicazione del genoma che è alla base della selezione naturale è essa stessa un prodotto dell&rsquo;evoluzione, l&rsquo;origine della vita deve essere spiegata al di fuori del quadro tradizionale della biologia evolutiva. La moderna teoria evolutiva, basata sulla genetica delle popolazioni, fornisce un resoconto dettagliato e probabilmente ampiamente soddisfacente dei processi microevolutivi, ovvero l&rsquo;evoluzione delle frequenze alleliche in una popolazione di organismi sottoposta a selezione e deriva genetica casuale (8, 9). Tuttavia, questa teoria ha poco da dire sull&rsquo;effettiva storia della vita, in particolare sull&rsquo;emergere di nuovi livelli di complessità biologica, e nulla sull&rsquo;origine della vita.</p><p>La caratteristica cruciale della complessità biologica è la sua organizzazione gerarchica. In effetti, la biologia è permeata da gerarchie a più livelli: dalle piccole molecole alle macromolecole; dalle macromolecole ai complessi funzionali, ai compartimenti subcellulari e alle cellule; dagli organismi unicellulari alle comunità, ai consorzi e alla multicellularità; dai semplici organismi multicellulari alle forme altamente complesse con tessuti differenziati; dagli organismi alle comunità e, infine, all&rsquo;eusocialità e alle biocenosi complesse coinvolte nei processi biogeochimici su scala planetaria. Tutti questi livelli distinti costituiscono insieme l&rsquo;organizzazione gerarchica della biosfera. Comprendere l&rsquo;origine e l&rsquo;evoluzione di questa complessità gerarchica è probabilmente uno degli obiettivi principali della biologia.</p><p>In gran parte, l&rsquo;evoluzione dell&rsquo;organizzazione multilivello dei sistemi biologici sembra essere guidata dalla soluzione di problemi di ottimizzazione, che comporta conflitti o compromessi tra criteri di ottimizzazione a diversi livelli o scale, portando a stati frustrati, nel linguaggio della fisica (10-12). Due casi notevoli sono la corsa agli armamenti parassita-ospite, che permea l&rsquo;evoluzione biologica e contribuisce in modo significativo alla diversità e alla complessità delle forme di vita (13-16), e l&rsquo;organizzazione multicellulare degli organismi complessi, in cui la tendenza delle singole cellule a riprodursi alla massima velocità possibile è contrastata dal controllo della divisione cellulare imposto a livello organismico (17, 18).
Due concetti fondamentali strettamente collegati ma distinti, che si collocano effettivamente al di fuori della narrazione canonica della biologia evolutiva, riguardano l&rsquo;evoluzione della complessità biologica: le transizioni principali nell&rsquo;evoluzione (MTE) (19-21) e la selezione multilivello (MLS) (22-27). Ogni MTE comporta l&rsquo;emergere di un nuovo livello di organizzazione, spesso descritto come una transizione evolutiva nell&rsquo;individualità. Un esempio lampante è l&rsquo;evoluzione della multicellularità, in cui emerge un nuovo livello di selezione, ossia la selezione tra insiemi di cellule piuttosto che tra singole cellule. Le forme di vita multicellulari (contando anche solo gli organismi complessi con più tipi di cellule) si sono evolute in molte occasioni indipendenti durante l&rsquo;evoluzione della vita (28, 29), il che implica che l&rsquo;emergere di nuovi livelli di complessità è una tendenza evolutiva importante piuttosto che un evento raro e casuale.</p><p>La MLS rimane un concetto controverso, presumibilmente a causa del legame con il tema a lungo dibattuto della selezione di gruppo (27, 30). Tuttavia, in quanto componente determinante dell&rsquo;MTE, il MLS sembra essere indispensabile. Un meccanismo generale proposto alla base dell&rsquo;MTE, formulato per analogia con la teoria fisica dell&rsquo;origine dei pattern (ad esempio, nei sistemi vetrosi), coinvolge interazioni concorrenti a diversi livelli e gli stati frustrati che tali interazioni causano (12). Nella teoria fisica degli occhiali a spin, le frustrazioni danno luogo alla non-ergodicità e consentono la formazione e la persistenza della memoria a lungo termine, cioè della storia (31, 32). Al contrario, i sistemi ergodici non hanno una vera storia perché raggiungono tutti gli stati possibili durante la loro evoluzione (almeno nel limite del tempo grande) e quindi l&rsquo;unico contenuto della quasi-storia di tali sistemi è la transizione da stati meno probabili a stati più probabili per ragioni puramente combinatorie: cioè l&rsquo;aumento di entropia (33). Come sottolineato nel libro fondamentale di Schroedinger (34), anche se solo in termini generali, la vita si basa su processi &ldquo;negentropici&rdquo;, e le frustrazioni a diversi livelli sono necessarie perché questi processi decollino e persistano (12).</p><p>L&rsquo;origine delle cellule, che può e probabilmente deve essere equiparata all&rsquo;origine della vita, è stata la prima e più importante transizione all&rsquo;inizio dell&rsquo;evoluzione biologica e, in quanto tale, esula dalle competenze della biologia evolutiva sensu stricto. Probabilmente, l&rsquo;indagine teorica sull&rsquo;origine della vita può essere possibile solo nell&rsquo;ambito di una teoria dell&rsquo;involucro che includa l&rsquo;evoluzione biologica come caso speciale. È naturale immaginare una teoria di questo tipo che comprenda tutti i processi non ergodici che avvengono nell&rsquo;universo, di cui la vita è un caso speciale, che emerge in condizioni ancora da indagare e definire.</p><p>Qui, alla ricerca di una teoria massimamente generale dell&rsquo;evoluzione, adottiamo il formalismo della teoria dell&rsquo;apprendimento automatico (35). È importante notare che qui l&rsquo;apprendimento è percepito in senso massimamente generale come un processo oggettivo che si verifica in tutti i sistemi in evoluzione, compresi, ma non solo, quelli biologici (36). Come tale, l&rsquo;analogia tra apprendimento e selezione appare ovvia. Entrambi i tipi di processi comportano prove ed errori e l&rsquo;accettazione o il rifiuto dei risultati sulla base di alcuni criteri formali; in altre parole, entrambi sono processi di ottimizzazione (22, 37, 38). Qui valutiamo fino a che punto si estende questa analogia, stabilendo la corrispondenza tra le caratteristiche chiave dell&rsquo;evoluzione biologica e i concetti e il formalismo matematico della teoria dell&rsquo;apprendimento. Sosteniamo che la funzione di perdita, che è centrale nella teoria dell&rsquo;apprendimento, può essere utilmente e generalmente impiegata come l&rsquo;equivalente della funzione di fitness nel contesto dell&rsquo;evoluzione. La nostra motivazione originaria era quella di spiegare le principali caratteristiche dell&rsquo;evoluzione biologica a partire da principi più generali della fisica. Tuttavia, dopo aver formulato tali principi e averli inseriti nel quadro matematico dell&rsquo;apprendimento, abbiamo scoperto che la teoria può potenzialmente applicarsi all&rsquo;intera storia dell&rsquo;universo in evoluzione (36), compresi i processi fisici che hanno avuto luogo fin dal big bang e i processi chimici che hanno direttamente preceduto e posto le basi per l&rsquo;origine della vita. Le proposizioni centrali della teoria dell&rsquo;evoluzione qui delineata includono sia i principi fisici chiave (cioè la gerarchia di scala, i gap di frequenza e la rinormalizzabilità) (39, 40) sia le principali caratteristiche della vita (come la MLS, la persistenza di parassiti genetici e la morte cellulare programmata).</p><p>Dimostriamo che l&rsquo;apprendimento in un ambiente complesso porta alla separazione delle scale, con le variabili addestrabili che si dividono in almeno due classi: quelle che cambiano più velocemente e quelle che cambiano più lentamente. Questa separazione di scala è alla base di tutti i processi che comportano la formazione di strutture complesse nell&rsquo;universo, dalla scala di un atomo a quella degli ammassi di galassie. Noi sosteniamo che, per l&rsquo;emergere della vita, sono essenziali almeno tre scale temporali, che corrispondono alle variabili ambientali, fenotipiche e genotipiche. Nei sistemi di apprendimento in evoluzione, le variabili che cambiano più lentamente vengono digitalizzate e acquisiscono la capacità di replicazione, dando luogo a una riproduzione differenziale a seconda del valore della funzione di perdita (fitness), necessaria e sufficiente per l&rsquo;inizio dell&rsquo;evoluzione per selezione naturale. La successiva evoluzione della vita comporta l&rsquo;emergere di molte scale aggiuntive, che corrispondono all&rsquo;MTE. In seguito, utilizzeremo il termine &ldquo;evoluzione&rdquo; per descrivere i cambiamenti temporali dei sistemi viventi, simili alla vita e prebiotici (organismi), mentre il termine più generale &ldquo;dinamica&rdquo; si riferisce ai processi temporali in altri sistemi fisici.</p><p>Almeno dalla pubblicazione del libro di Schroedinger, è stata discussa la possibilità che, sebbene la vita obbedisca certamente alle leggi della fisica, possa esistere una classe di leggi diversa, unica per la biologia. Spesso questa presunta fisica della vita viene associata all&rsquo;emergenza (41-43), ma la natura dei fenomeni emergenti coinvolti, a nostra conoscenza, non è stata chiarita fino a poco tempo fa (36). Qui delineiamo un approccio generale alla modellazione e allo studio dell&rsquo;evoluzione come apprendimento multilivello, sostenendo l&rsquo;idea che un tipo distinto di teoria fisica, vale a dire la teoria dell&rsquo;apprendimento (35, 36), sia necessario per indagare l&rsquo;evoluzione degli oggetti complessi nell&rsquo;universo, di cui l&rsquo;evoluzione della vita è una forma specifica, anche se altamente notevole.</p><h2 id=1-principi-fondamentali-dellevoluzione>1. Principi fondamentali dell&rsquo;evoluzione</h2><p>In questa sezione cerchiamo di formulare i principi universali minimi che definiscono un universo osservabile, in cui l&rsquo;evoluzione è possibile e forse inevitabile. La nostra analisi parte dalle principali caratteristiche dell&rsquo;evoluzione biologica discusse nella sezione successiva e procede verso i principi generali. Tuttavia, per motivi di trasparenza e generalità, iniziamo la discussione con questi ultimi.</p><p>Quali sono i requisiti perché un universo sia osservabile? La possibilità di fare osservazioni significative implica un certo grado di ordine e di complessità nell&rsquo;universo osservato che emerge da processi evolutivi, e tale evolvibilità sembra essere di per sé predicata da diversi principi fondamentali. Va sottolineato che &ldquo;osservazione&rdquo; e &ldquo;apprendimento&rdquo; non implicano affatto &ldquo;mente&rdquo; o &ldquo;coscienza&rdquo;, ma un requisito molto più basilare. Per imparare e sopravvivere in un ambiente, un sistema (o un osservatore) deve prevedere, con un grado minimo ma sufficiente di accuratezza, la risposta di quell&rsquo;ambiente a varie azioni ed essere in grado di scegliere tali azioni che siano compatibili con la permanenza dell&rsquo;osservatore in quell&rsquo;ambiente. In questo senso, qualsiasi forma di vita è un osservatore, e lo sono anche i sistemi inanimati dotati della capacità di reagire in modo retroattivo. In questo senso più generale, l&rsquo;osservazione è un prerequisito per l&rsquo;evoluzione. Formuliamo prima i principi di base dell&rsquo;osservabilità e dell&rsquo;evolvibilità e poi forniamo i commenti e le spiegazioni pertinenti.</p><ul><li>P1. Funzione di perdita. In qualsiasi sistema in evoluzione, esiste una funzione di perdita di variabili dipendenti dal tempo che viene minimizzata durante l&rsquo;evoluzione.</li><li>P2. Gerarchia di scale. I sistemi in evoluzione comprendono più variabili dinamiche che cambiano su scale temporali diverse (con frequenze caratteristiche diverse).</li><li>P3. Lacune di frequenza. Le variabili dinamiche sono suddivise tra livelli distinti di organizzazione separati da intervalli di frequenza sufficientemente ampi.</li><li>P4. Rinormalizzabilità. Nell&rsquo;intera gamma di organizzazione dei sistemi in evoluzione, una descrizione statistica delle variabili a più rapida variazione (a più alta frequenza) è possibile attraverso le variabili a più lenta variazione (a più bassa frequenza).</li><li>P5. Estensione. I sistemi in evoluzione hanno la capacità di reclutare ulteriori variabili che possono essere utilizzate per sostenere il sistema e la capacità di escludere le variabili che potrebbero destabilizzare il sistema.</li><li>P6. Replicazione. Nei sistemi in evoluzione, la replicazione e l&rsquo;eliminazione delle corrispondenti unità di elaborazione delle informazioni (UIP) possono avvenire a ogni livello di organizzazione.</li><li>P7. Flusso di informazioni. Nei sistemi in evoluzione, i livelli che cambiano più lentamente assorbono le informazioni dai livelli che cambiano più velocemente durante l&rsquo;apprendimento e le trasmettono ai livelli più veloci per prevedere lo stato dell&rsquo;ambiente e del sistema stesso.</li></ul><p>Il primo principio (P1) è di particolare importanza come punto di partenza per una descrizione formale dell&rsquo;evoluzione come processo di apprendimento. L&rsquo;esistenza stessa di una funzione di perdita implica che il sistema dinamico dell&rsquo;universo o, più semplicemente, l&rsquo;universo stesso è un sistema che apprende (evolve) (36). In effetti, qui assumiamo che la stabilità o la sopravvivenza di qualsiasi sottosistema dell&rsquo;universo equivalga alla soluzione di un problema di ottimizzazione o di apprendimento in senso matematico e che ci sia sempre qualcosa da imparare. In particolare, per risolvere problemi di ottimizzazione complessi che dipendono da molte variabili, il metodo migliore e di fatto l&rsquo;unico efficiente è la selezione implementata in vari algoritmi stocastici (Markov Chain Monte Carlo, discesa stocastica del gradiente, algoritmi genetici e altri). Tutta l&rsquo;evoluzione può essere percepita come un&rsquo;implementazione di un algoritmo di apprendimento stocastico. In altre parole, l&rsquo;apprendimento è un&rsquo;ottimizzazione per tentativi ed errori, così come l&rsquo;evoluzione.</p><p>I restanti principi da P2 a P7 forniscono le condizioni sufficienti affinché gli osservatori del nostro tipo (cioè le forme di vita complesse) si evolvano all&rsquo;interno di un sistema di apprendimento. In particolare, P2, P3 e P4 comprendono le condizioni necessarie per l&rsquo;osservabilità di un universo da parte di qualsiasi osservatore, mentre P5, P6 e P7 rappresentano le condizioni necessarie per l&rsquo;origine della vita del nostro tipo (di seguito, omettiamo la qualifica per brevità). Più precisamente, P2 e P3 prevedono la possibilità di una forma almeno semplice di apprendimento dell&rsquo;ambiente (variabili che cambiano velocemente) da parte di un osservatore (variabili che cambiano lentamente) e quindi l&rsquo;emergere di un&rsquo;organizzazione complessa delle variabili che cambiano lentamente. P4 corrisponde al concetto fisico di rinormalizzabilità, o gruppo di rinormalizzazione (39, 40), in base al quale le stesse equazioni macroscopiche, anche se con parametri diversi, governano processi a livelli o scale differenti, limitando così il numero di variabili rilevanti, vincolando la complessità e consentendo una descrizione a grana grossa. Questo principio garantisce un universo rinormalizzabile, capace di evolversi e di essere osservato. Insieme, P2 e P4 definiscono un universo in cui la conoscenza parziale o approssimativa dell&rsquo;ambiente (in altre parole, la grana grossa) è raggiungibile e utile per la sopravvivenza dei sistemi in evoluzione (osservatori). In un universo in cui P4 non si applica (cioè un universo con leggi fisiche non normalizzabili), ciò che accade a livello macroscopico dipenderà in modo critico dai dettagli dei processi a livello microscopico. In un universo in cui P2 e P3 non si applicano, la stessa separazione tra micro e macrolivello non sarebbe evidente. In un universo del genere, sarebbe impossibile sopravvivere senza aver prima scoperto le leggi fisiche fondamentali, mentre gli organismi viventi sul nostro pianeta si sono evoluti per miliardi di anni prima di iniziare a studiare la fisica quantistica.</p><p>I principi P5, P6 e P7 danno ai sistemi in evoluzione l&rsquo;accesso ad algoritmi più avanzati per l&rsquo;apprendimento e la previsione dell&rsquo;ambiente, aprendo la strada all&rsquo;evoluzione di sistemi complessi, compresa, infine, la vita. Questi principi sono alla base dell&rsquo;emergere del fenomeno cruciale della selezione (44, 45). Nella sua forma più semplice, la selezione è finalizzata alla stabilità e alla persistenza dei sistemi in evoluzione e in apprendimento (46). L&rsquo;apprendimento e la sopravvivenza sono strettamente legati perché la sopravvivenza dipende dalla capacità del sistema di estrarre informazioni dall&rsquo;ambiente e questa capacità dipende dalla stabilità del sistema sui tempi necessari per l&rsquo;apprendimento. Grosso modo, un sistema non può sopravvivere in un mondo in cui le proprietà dell&rsquo;ambiente cambiano più velocemente di quanto il sistema in evoluzione possa apprendere. Secondo P5, i sistemi in evoluzione consumano risorse (come il cibo), che a loro volta potrebbero essere prodotte da altri sistemi in evoluzione, per essere utilizzate come blocchi di costruzione e fonti di energia, necessarie per l&rsquo;apprendimento. Questo principio incarna la visione di Schroedinger secondo cui &ldquo;gli organismi si nutrono di negentropia&rdquo; (34). In base a P6, la replica dei vettori di variabili che cambiano lentamente diventa la base della persistenza a lungo termine e della memoria nei sistemi in evoluzione. Questo principio può essere visto come un algoritmo di apprendimento costruito su P3, in cui i tempi caratteristici di un singolo organismo e di generazioni consecutive sono separati. Questo principio esclude dalla considerazione alcune forme di vita immaginarie: ad esempio, il famoso Solaris di Stanislav Lem (47). Infine, P7 descrive come l&rsquo;informazione fluisca tra i diversi livelli dell&rsquo;apprendimento multilivello, dando origine a un dogma centrale generalizzato della biologia molecolare, discusso in Dogma centrale generalizzato della biologia molecolare.</p><h2 id=2-fenomeni-evolutivi-fondamentali>2. Fenomeni evolutivi fondamentali</h2><p>In questa sezione, colleghiamo i principi fondamentali dell&rsquo;evoluzione da P1 a P7 formulati sopra alle caratteristiche fenomenologiche di base della vita (da E1 a E10) e cerchiamo le equivalenze nella teoria dell&rsquo;apprendimento. L&rsquo;elenco che segue è organizzato formulando prima una caratteristica biologica, e poi è organizzato 1) tracciando le connessioni con i principi fondamentali e 2) aggiungendo commenti più generali.</p><h3 id=e1-ipu>E1. IPU.</h3><p>Le IPU discrete (cioè la differenziazione e la discriminazione tra sé e non sé) esistono a tutti i livelli di organizzazione. Tutti i sistemi biologici a tutti i livelli di organizzazione, come geni, cellule, organismi, popolazioni e così via fino al livello dell&rsquo;intera biosfera, possiedono un certo grado di autocoerenza che li separa, innanzitutto, dall&rsquo;ambiente in generale e da altre UIP di livello simile.</p><ol><li><p>L&rsquo;esistenza delle UIP si basa sui principi fondamentali da P1 a P4. L&rsquo;ampia gamma di scale temporali (P2) nei sistemi dinamici e le lacune tra le scale (P3) consentono naturalmente di separare le componenti che cambiano più lentamente da quelle che cambiano più velocemente. In particolare, la rinormalizzabilità (P4) si applica alla gerarchia delle IPU. La prevedibilità statistica delle frequenze più alte permette alle UIP di diminuire la funzione di perdita delle frequenze più basse, nonostante i tempi di reazione molto più lenti.</p></li><li><p>La separazione delle UIP prebiologiche a cambiamento (relativamente) lento dall&rsquo;ambiente a cambiamento (tipicamente) rapido ha dato il via alla forma più primitiva di selezione prebiologica: la selezione per la stabilità e la persistenza (survivor bias). Le UIP più stabili e a più lento cambiamento vincono la competizione e si accumulano nel tempo, aumentando la separazione lungo l&rsquo;asse temporale man mano che il confine tra le UIP e l&rsquo;ambiente diventa più netto. Altri fenomeni chiave, come l&rsquo;utilizzo delle risorse ambientali disponibili (P5) e la modalità di scambio di informazioni stimolo-risposta (P7), derivano dal flusso di materia e informazioni attraverso questo confine e dalla conseguente separazione dei processi fisico-chimici interni ed esterni. La crescente differenziazione tra sé e non sé, combinata con la replicazione dei vettori di variabili a lento cambiamento (P6), pone le basi per la competizione tra entità in evoluzione e per l&rsquo;insorgere del fenomeno evolutivo per eccellenza, la selezione naturale (E6).</p></li></ol><h3 id=e2-frustrazione>E2. Frustrazione.</h3><p>Tutti i sistemi dinamici complessi devono affrontare problemi di ottimizzazione multidimensionale e multiscala, che genericamente portano alla frustrazione derivante da obiettivi in conflitto su scale diverse. Questa è una caratteristica intrinseca di tutti i sistemi di questo tipo e una delle principali forze che guidano l&rsquo;avvento della crescente complessità multilivello (12). La frustrazione è un fenomeno fisico estremamente generale che non è affatto limitato alla biologia, ma si verifica già in sistemi fisici molto più semplici, come gli spin e i vetri strutturali, il cui comportamento è determinato da interazioni in competizione tra loro in modo da raggiungere un certo grado di complessità (31, 32).</p><ol><li><p>L&rsquo;organizzazione multiscala dell&rsquo;universo (P2) fornisce le basi fisiche per l&rsquo;ubiquità degli stati frustrati, che si verificano tipicamente ogni volta che c&rsquo;è un conflitto (trade-off) tra problemi di ottimizzazione a breve e a lungo raggio. Le interazioni frustrate producono paesaggi di potenziale a più livelli, in cui nessun singolo stato è sostanzialmente più adatto di numerosi altri optima locali. L&rsquo;ottimizzazione multiparametrica e multiscala della funzione di perdita su un tale paesaggio comporta una dinamica non ergodica (dipendente dalla storia), caratteristica dei sistemi complessi.</p></li><li><p>Le IPU devono affrontare interazioni conflittuali a partire dallo stato prebiologico più primitivo (12). Infatti, la separazione di qualsiasi sistema dall&rsquo;ambiente comporta immediatamente un conflitto di permeabilità; una separazione più forte aumenta la differenziazione tra sé e non sé e, quindi, aumenta la stabilità del sistema, ma compromette lo scambio di informazioni e di materia con l&rsquo;ambiente, limitando il potenziale di crescita. In biologia, praticamente tutti gli aspetti dell&rsquo;architettura e del funzionamento dell&rsquo;organismo sono soggetti a tali frustrazioni o compromessi: il conflitto tra la fedeltà e la velocità della trasmissione delle informazioni a tutti i livelli, tra la specializzazione e il generalismo, tra i benefici a livello individuale e quelli a livello di popolazione, e altro ancora. L&rsquo;ubiquità delle frustrazioni e l&rsquo;impossibilità fondamentale di risolverle in modo universalmente ottimale sono motori perpetui dell&rsquo;evoluzione e danno luogo a transizioni evolutive, raggiungendo livelli di complessità altrimenti irraggiungibili.</p></li></ol><p>Esistono due tipi distinti di frustrazione, quella spaziale e quella temporale. La frustrazione spaziale è simile a quella comunemente analizzata nei sistemi di materia condensata, come i vetri di spin (31, 32). In questo caso, i termini interagenti spazialmente locali e non locali hanno segni opposti e lo stato di equilibrio è determinato dall&rsquo;equilibrio tra i termini. Nelle reti neurali, un neurone (come un singolo spin) può avere un obiettivo locale (come la classificazione binaria dei segnali in arrivo) ma è anche parte di una rete neurale (come una rete di spin), che ha un proprio obiettivo globale (come la previsione delle condizioni al contorno). Per un particolare neurone, l&rsquo;ottimizzazione dell&rsquo;obiettivo locale può entrare in conflitto con l&rsquo;obiettivo globale, causando una frustrazione spaziale. La frustrazione temporale emerge perché, nel contesto dell&rsquo;apprendimento multilivello, lo stesso neurone diventa parte di IPU di livello superiore che operano su scale temporali (frequenze) diverse. Quindi, lo stato ottimale del neurone rispetto a un&rsquo;IPU che opera a una determinata scala temporale può differire dallo stato ottimale dello stesso neurone rispetto a un&rsquo;altra IPU che opera a una scala temporale diversa (36). Come le frustrazioni spaziali, anche quelle temporali non possono essere completamente risolte, ma un equilibrio ottimale tra le diverse scale spaziali e temporali è raggiungibile e rappresenta un equilibrio locale del sistema di apprendimento.</p><h3 id=e3-gerarchia-multilivello>E3. Gerarchia multilivello.</h3><p>La gerarchia di più livelli di organizzazione è una caratteristica intrinseca ed essenziale dei sistemi biologici in evoluzione, sia per quanto riguarda la struttura di questi sistemi (geni, genomi, cellule, organismi, gruppi di parentela, popolazioni, specie, comunità e altro) sia per quanto riguarda il substrato su cui agiscono le forze evolutive.</p><ol><li><p>La rinormalizzabilità dell&rsquo;universo (P4) implica che non esiste un livello di organizzazione intrinsecamente preferito, per il quale tutto ciò che sta sopra e sotto si comporterebbe come un insieme omogeneo. Anche se alcuni livelli di organizzazione nascono prima di altri (per esempio, gli organismi prima dei geni o gli organismi unicellulari prima di quelli multicellulari), gli altri livelli emergeranno e si consolideranno necessariamente in seguito.</p></li><li><p>La gerarchia dell&rsquo;organizzazione strutturale dei sistemi biologici era evidente agli studiosi fin dai primi tempi della scienza. Tuttavia, la MLS è stata e rimane un argomento controverso nella biologia evolutiva (23, 26, 27). Intuitivamente e come implicito nell&rsquo;equazione di Price (48), il MLS dovrebbe emergere in tutti i sistemi evolutivi finché l&rsquo;agenzia di selezione di livello superiore possiede un grado sufficiente di differenziazione tra sé e non sé. In particolare, se gli organismi di una data specie formano popolazioni sufficientemente distinte dal punto di vista genetico e interagiscono in modo competitivo, si avrà una selezione a livello di popolazione. L&rsquo;evoluzione dei sistemi biologici è guidata da interazioni conflittuali (E2) che tendono a portare a una complessità sempre maggiore (12). Questa tendenza alimenta ulteriormente la propensione di questi sistemi a formare nuovi livelli di organizzazione ed è associata a transizioni evolutive che comportano l&rsquo;avvento di nuove unità di selezione a più livelli di complessità. Pertanto, l&rsquo;E3 può essere considerata una conseguenza importante dell&rsquo;E2.</p></li></ol><h3 id=e4-quasi-ottimizzazione>E4. Quasi-ottimizzazione.</h3><p>L&rsquo;ottimizzazione stocastica o l&rsquo;uso di algoritmi di ottimizzazione stocastica è l&rsquo;unico approccio fattibile all&rsquo;ottimizzazione complessa, ma non garantisce né la ricerca della soluzione globalmente ottimale né il mantenimento della configurazione ottimale quando e se viene trovata. Piuttosto, l&rsquo;ottimizzazione stocastica tende a trovare rapidamente degli ottimi locali e a mantenere il sistema nelle loro vicinanze, mantenendo il valore della funzione di perdita a un livello quasi ottimale.</p><ol><li><p>Secondo P1, la dinamica di un sistema che apprende (cioè che si auto-ottimizza) è definita da una funzione di perdita (35, 36). In presenza di un forte gradiente nella funzione di perdita, un sistema in fase di ottimizzazione stocastica scende rapidamente nella direzione giusta. Tuttavia, a causa delle frustrazioni che inevitabilmente derivano dalle interazioni in un sistema complesso, i picchi locali effettivi sul paesaggio sono raramente raggiunti e il picco globale è di fatto irraggiungibile. I sistemi di apprendimento tendono a bloccarsi in prossimità di punti di sella locali in cui i cambiamenti lungo la maggior parte delle dimensioni portano &ldquo;in alto&rdquo; o sono &ldquo;piatti&rdquo; in termini di funzione di perdita, con solo una piccola minoranza delle mosse disponibili che diminuiscono la funzione di perdita (49).</p></li><li><p>I sistemi biologici attuali (cellule, organismi multicellulari ed entità di livello superiore, come popolazioni e comunità) sono il prodotto di circa 4 miliardi di anni di evoluzione della vita e, come tali, sono altamente ottimizzati, anche se non completamente. Di conseguenza, la distribuzione tipica degli effetti dei cambiamenti ereditabili nell&rsquo;evoluzione biologica comprende numerosi cambiamenti deleteri, cambiamenti benefici relativamente rari e cambiamenti neutri comuni, nonché quelli con effetti di fitness inferiori al livello di rumore (50). La preponderanza di cambiamenti neutri e leggermente deleteri consente l&rsquo;evoluzione per deriva genetica, in base alla quale una popolazione si sposta sullo stesso livello o addirittura leggermente verso il basso nel paesaggio della fitness, raggiungendo potenzialmente un&rsquo;altra regione del paesaggio in cui sono disponibili mutazioni benefiche (51, 52).</p></li></ol><h3 id=e5-diversità-delle-soluzioni-quasi-ottimali>E5. Diversità delle soluzioni quasi ottimali.</h3><p>Le soluzioni sui paesaggi delle funzioni di perdita che si presentano nei problemi di ottimizzazione complessi comprendono numerosi picchi locali di altezza comparabile.</p><ol><li><p>L&rsquo;esistenza di più picchi di altezza comparabile nei paesaggi delle funzioni di perdita è una proprietà fisica fondamentale dei sistemi frustrati (E2), mentre la pervasività della frustrazione stessa è una conseguenza dell&rsquo;organizzazione multiscala e multilivello dell&rsquo;universo (P2). I sistemi dinamici frustrati sono non ergodici, il che, dal punto di vista biologico, significa che, una volta separati, le traiettorie evolutive divergono anziché convergere. Poiché la maggior parte di queste traiettorie attraversa parti dello spazio genotipico con valori di fitness comparabili, la competizione raramente si traduce in una completa dominanza di un lignaggio sugli altri, ma piuttosto genera una ricca diversità.</p></li><li><p>In termini di biologia evolutiva, i paesaggi di fitness sono accidentati, con più picchi adattativi di fitness comparabile (53, 54), e una tendenza saliente durante l&rsquo;evoluzione è la diffusione delle forme di vita su più picchi, invece di concentrarsi su uno o pochi. L&rsquo;evoluzione spinge gli organismi in evoluzione a esplorare e occupare tutte le nicchie disponibili e a provare tutte le strategie possibili. Nel contesto dell&rsquo;apprendimento automatico, reti neurali identiche possono partire dallo stesso stato iniziale ma, ad esempio, con l&rsquo;algoritmo di discesa stocastica del gradiente, evolverebbero genericamente verso minimi locali diversi. Pertanto, la diversità delle soluzioni è una proprietà generica dei sistemi di apprendimento. Più tecnicamente, la diversificazione è dovuta alla produzione di entropia attraverso la dinamica delle variabili neutre addestrabili (si veda la prossima sezione).</p></li></ol><h3 id=e6-separazione-del-fenotipo-dal-genotipo>E6. Separazione del fenotipo dal genotipo.</h3><p>Questa caratteristica quintessenziale della vita incarna due distinti (anche se inseparabili negli organismi conosciuti) fenomeni di rottura della simmetria: 1) la separazione tra supporti digitali dedicati all&rsquo;immagazzinamento dell&rsquo;informazione (stabili, raramente aggiornabili, tendenti a distribuzioni con valori discreti) e dispositivi di elaborazione per lo più analogici e 2) l&rsquo;asimmetria del flusso di informazioni all&rsquo;interno delle UIP, in base alla quale il genotipo fornisce &ldquo;istruzioni&rdquo; per il fenotipo, mentre il fenotipo perde in gran parte la capacità di aggiornare direttamente il genotipo. La separazione tra i sottosistemi di immagazzinamento ed elaborazione delle informazioni è un prerequisito per un&rsquo;evoluzione efficiente, che probabilmente è emersa presto nel percorso che porta dalle entità prebiotiche alla comparsa della vita.</p><ol><li><p>La separazione tra fenotipo e genotipo estende la separazione di scala a livello intra-IPU come segue dai principi fondamentali da P1 a P4. Le componenti a frequenza intermedia di un&rsquo;UIP (fenotipo) proteggono le componenti più lente dall&rsquo;interazione diretta con l&rsquo;ambiente (le variabili a più alta frequenza), aumentando ulteriormente la stabilità delle componenti più lente e rendendole adatte alla memorizzazione di informazioni a lungo termine. Quando le scale temporali si separano ulteriormente, le interazioni tra di esse cambiano. Il flusso asimmetrico di informazioni (P7) stabilizza il sistema, consentendo la conservazione a lungo termine delle informazioni (genotipo) e mantenendo la flessibilità reattiva dei componenti che cambiano più rapidamente (fenotipo).</p></li><li><p>L&rsquo;emergere della separazione tra fenotipo e genotipo è un evento cruciale nell&rsquo;evoluzione prebiotica. Questa separazione è evidente in tutte le forme di vita conosciute e ipotetiche. Anche quando i ruoli di fenotipo e genotipo sono svolti da molecole chimicamente identiche, come nello scenario del mondo a RNA dell&rsquo;evoluzione primordiale (55, 56), i loro ruoli come effettori e dispositivi di immagazzinamento delle informazioni sono nettamente distinti. In termini biologici, la divisione è tra replicatori (cioè i portatori di informazioni digitali [genomi]) e riproduttori (57-59), i dispositivi analogici (cellule, organismi) che ospitano i replicatori, li riforniscono di blocchi di costruzione (P5) e si riproducono a loro volta (P6) sotto le istruzioni dei replicatori (P7). Sebbene la separazione genotipo/fenotipo sia un elemento fondamentale della vita, di per sé non è sufficiente a qualificare un&rsquo;UIP come forma di vita (i computer e i giradischi, in cui la separazione tra le parti di memorizzazione dell&rsquo;informazione e quelle operative è prominente ed essenziale, chiaramente non sono vita, anche se inventati da organismi avanzati). L&rsquo;asimmetria del flusso di informazioni tra genotipo e fenotipo (P7) è la forma più generale del fenomeno noto come Dogma Centrale della biologia molecolare: il flusso unidirezionale di informazioni dagli acidi nucleici alle proteine, come originariamente formulato da Crick (60). Questa asimmetria è evidente anche in altri sistemi di elaborazione dell&rsquo;informazione, in particolare nei computer. Infatti, i computer con architettura di von Neumann hanno unità di memoria e di elaborazione intrinsecamente distinte, con un flusso di istruzioni dalla prima alla seconda (61, 62). Sembra che tutti i sistemi avanzati di elaborazione dell&rsquo;informazione siano dotati di questa proprietà.</p></li></ol><h3 id=e7-replicazione>E7. Replicazione.</h3><p>L&rsquo;emergere di dispositivi di memorizzazione digitale a lungo termine, cioè genomi costituiti da RNA o DNA (E6), consente di conservare le informazioni a lungo termine, facilita le reazioni adattative ai cambiamenti dell&rsquo;ambiente e promuove la stabilità delle unità di elaborazione dell&rsquo;informazione fino al punto in cui (almeno nei sistemi chimici) è limitata dall&rsquo;energia dei legami chimici piuttosto che dall&rsquo;energia delle fluttuazioni termiche. Ovviamente, però, finché questa informazione è confinata a un&rsquo;unica UIP, sparirà con l&rsquo;inevitabile distruzione finale di quell&rsquo;UIP. In questo caso, altre IPU di architettura simile dovrebbero accumulare da zero una quantità di informazioni paragonabile per raggiungere lo stesso livello di stabilità. Pertanto, la copia e la condivisione delle informazioni sono essenziali per la persistenza a lungo termine (di fatto, indefinita) delle UIP.</p><ol><li><p>Il principio fondamentale P6 postula l&rsquo;esistenza di meccanismi di copia ed eliminazione delle informazioni. Se l&rsquo;informazione genomica può essere replicata, anche i meccanismi di condivisione più primitivi (come la scissione fisica di un&rsquo;UIP sotto la forza della tensione superficiale) porterebbero (anche se non in modo affidabile) all&rsquo;emergere di UIP distinte e precaricate di informazioni accumulate dai loro progenitori. Questo processo manda in cortocircuito l&rsquo;apprendimento e permette all&rsquo;informazione di accumularsi su tempi che superano di gran lunga i tempi di vita caratteristici delle singole UIP.</p></li><li><p>La copia e la condivisione di informazioni sono vantaggiose solo se la fedeltà supera una certa soglia, talvolta chiamata limite di Eigen in biologia evolutiva (5-7). Tuttavia, nei sistemi prebiotici primitivi, il livello di fedeltà richiesto poteva essere piuttosto basso (63). Per esempio, anche una composizione chimica distorta di una gocciolina idrofobica potrebbe aumentare la stabilità delle goccioline discendenti e quindi dotarle di un vantaggio nella selezione per la persistenza. Tuttavia, quando emergono meccanismi relativamente sofisticati di copia e condivisione delle informazioni o, più precisamente, quando i replicatori diventano dispositivi di immagazzinamento delle informazioni, la stabilità complessiva del sistema può aumentare di ordini di grandezza. Per dirla in modo sorprendente, l&rsquo;unica biosfera a noi nota rappresenta una catena ininterrotta di trasmissione di informazioni genetiche che si estende per circa 4 miliardi di anni, commisurata alla scala dell&rsquo;evoluzione stellare.</p></li></ol><h3 id=e8-selezione-naturale>E8. Selezione naturale.</h3><p>L&rsquo;evoluzione per selezione naturale (evoluzione darwiniana) nasce dalla combinazione di tutti i principi e i fenomeni descritti sopra. Le condizioni necessarie e sufficienti per il funzionamento dell&rsquo;evoluzione darwiniana sono: 1) l&rsquo;esistenza di UIP distinte dall&rsquo;ambiente e l&rsquo;una dall&rsquo;altra (E1), 2) la dipendenza della stabilità di un&rsquo;UIP dall&rsquo;informazione che contiene (cioè il feedback fenotipo-genotipo; E6) e 3) la capacità delle UIP di creare copie dell&rsquo;informazione incorporata e di condividerla con altre UIP (E7). Quando queste tre condizioni sono soddisfatte, le frequenze relative delle UIP più stabili aumenteranno nel tempo attraverso il logoramento di quelle meno stabili (sopravvivenza del più adatto) e il trasferimento di informazioni tra le UIP, sia verticalmente (alla progenie) sia orizzontalmente. Questo processo genera la caratteristica chiave dell&rsquo;evoluzione darwiniana, la riproduzione differenziale dei genotipi, basata sul feedback dell&rsquo;ambiente trasmesso attraverso il fenotipo.</p><ol><li><p>Tutti e sette i principi fondamentali degli universi compatibili con la vita (da P1 a P7) sono coinvolti nell&rsquo;evoluzione per selezione naturale. L&rsquo;esistenza stessa delle unità, su cui può operare la selezione, dipende dalla discriminazione tra sé e non sé delle UIP prebiotiche (E1) e dall&rsquo;emergere dell&rsquo;immagazzinamento di informazioni condivisibili (E6 ed E7). Il passo cruciale per la biologia è l&rsquo;emergere del legame tra la funzione di perdita (P1), da un lato, e l&rsquo;esistenza delle UIP (P2, P3, P4 ed E1), dall&rsquo;altro. Il consumo di risorse esterne (limitate) (P5) comporta la competizione tra le UIP che condividono lo stesso ambiente e trasforma i semplici spostamenti delle frequenze relative in una vera e propria &ldquo;sopravvivenza del più adatto&rdquo;. La capacità delle UIP di replicarsi (P6) e di espandere la loro memoria (genotipo; P7, E6 ed E7) dà loro accesso a gradi di libertà finora non disponibili, rendendo l&rsquo;evoluzione un processo aperto piuttosto che una ricerca rapida e limitata di un optimum locale.</p></li><li><p>L&rsquo;evoluzione per selezione naturale è il principio centrale della biologia evolutiva e una parte fondamentale della definizione di vita della NASA. Una nota importante sulle definizioni è doverosa. Abbiamo già fatto riferimento alla selezione quando abbiamo parlato dell&rsquo;evoluzione prebiotica (E1); tuttavia, il termine &ldquo;selezione naturale (darwiniana)&rdquo; è qui riservato alla forma efficiente di selezione che emerge con la replicazione di dispositivi dedicati all&rsquo;immagazzinamento delle informazioni (P6 e E6). La riproduzione differenziale, in cui l&rsquo;ambiente fornisce un feedback sull&rsquo;idoneità dei genotipi mentre agisce sui fenotipi, si trasforma nella darwiniana sopravvivenza del più adatto in presenza di competizione. Quando le IPU dipendono dalle risorse ambientali, tale competizione si verifica inevitabilmente, tranne nel caso irrealistico di una fornitura illimitata (44). Con l&rsquo;inizio dell&rsquo;evoluzione darwiniana, si può considerare che il sistema attraversi la soglia dalla pre-vita alla vita (64, 65). Il processo evolutivo è naturalmente rappresentato dal movimento di un&rsquo;IPU in evoluzione in uno spazio genotipico, dove la prossimità è definita dalla somiglianza tra genotipi distinti e le transizioni corrispondono a eventi evolutivi elementari: cioè mutazioni nel senso più generale (66). Per ogni ambiente, l&rsquo;idoneità, cioè la misura della capacità di un genotipo di produrre prole vitale, può essere definita per ogni punto dello spazio genotipico, formando un paesaggio di idoneità multidimensionale (53, 54). La selezione crea un bias per la fissazione preferenziale delle mutazioni che aumentano la fitness, anche se le mutazioni stesse si verificano in modo completamente casuale.</p></li></ol><h3 id=e9-parassitismo>E9. Parassitismo.</h3><p>I parassiti e la coevoluzione ospite-parassita sono onnipresenti nei sistemi biologici a più livelli di organizzazione e sono intrinseci e indispensabili per l&rsquo;evoluzione della vita.</p><ol><li><p>A causa della flessibilità dei sistemi compatibili con la vita (P5 e P6) e della rottura della simmetria nel flusso di informazioni (P7), combinata con la tendenza intrinseca della vita a diversificarsi (E5), alcune parti del sistema inevitabilmente si stabiliscono in uno stato di parassitismo: cioè, rubano informazioni e materia dall&rsquo;ospite senza dare un contributo positivo alla sua fitness.</p></li><li><p>Dal punto di vista biologico, i parassiti si evolvono per ridurre al minimo la loro interfaccia diretta con l&rsquo;ambiente e, al contrario, per massimizzare la loro interazione con l&rsquo;ospite; in altre parole, l&rsquo;ospite sostituisce la maggior parte dell&rsquo;ambiente al parassita. I parassiti emergono e persistono inevitabilmente nei sistemi biologici per due motivi. 1) lo stato parassitario è raggiungibile attraverso una fase di aumento dell&rsquo;entropia e quindi è altamente probabile (16); 2) l&rsquo;immunità antiparassitaria altamente efficiente è costosa (67). Il costo dell&rsquo;immunità riflette un altro trade-off universale analogo a quello tra la fedeltà del trasferimento di informazioni e il dispendio energetico; in entrambi i casi, è necessaria una quantità infinita di energia per raggiungere un tasso di errore pari a zero o uno stato privo di parassiti. Da un punto di vista complementare, i parassiti si evolvono inevitabilmente come imbroglioni nel gioco della vita che sfruttano l&rsquo;ospite come risorsa, senza spendere energia per la produzione di risorse. A breve termine, i parassiti riducono la fitness dell&rsquo;ospite sia per il drenaggio diretto delle sue risorse sia per vari effetti indiretti, tra cui il costo della difesa. Tuttavia, in una prospettiva a lungo termine, i parassiti costituiscono un serbatoio per il reclutamento di nuove funzioni (soprattutto, ma non esclusivamente, per la difesa) da parte degli ospiti (14, 15). La relazione ospite-parassita può evolvere verso la transizione a uno stile di vita simbiotico e reciprocamente vantaggioso, che può ulteriormente progredire fino al mutualismo e, in alcuni casi, alla completa integrazione, come esemplificato dall&rsquo;origine degli organelli endosimbiotici essenziali negli eucarioti, i mitocondri e i cloroplasti (68, 69). I parassiti emergono a livelli simili di organizzazione biologica (organismi che parassitano altri organismi) o a livelli diversi (elementi genetici che parassitano genomi di organismi o cloni cellulari che parassitano organismi multicellulari).</p></li></ol><h3 id=e10-morte-programmata>E10. Morte programmata.</h3><p>La morte programmata (a vari livelli) è una caratteristica intrinseca della vita.</p><ol><li><p>La replicazione e l&rsquo;eliminazione delle IPU (P6) e l&rsquo;utilizzo di ulteriori gradi di libertà (P5) costituiscono la base del fenomeno della morte programmata. Ad alcuni livelli di organizzazione (ad esempio, intragenomica), la capacità di aggiungere ed eliminare unità (come i geni) a beneficio dei sistemi di livello superiore (come gli organismi) fornisce un ovvio percorso di ottimizzazione. L&rsquo;eliminazione delle unità potrebbe essere, in linea di principio, completamente casuale, ma la selezione (E8) genera un feedback sufficientemente forte per facilitare e strutturare il processo di perdita (ad esempio, l&rsquo;eliminazione dei geni a bassa idoneità attraverso la ricombinazione omologa o il suicidio altruistico di cellule infette o comunque compromesse). Le stesse forze operano almeno a livello cellulare e, plausibilmente, a tutti i livelli di organizzazione e selezione (P4). In particolare, se la selezione a livello di popolazione o di parentela è sufficientemente forte, i meccanismi per la morte altruistica dei singoli organismi possono apparentemente essere fissati nell&rsquo;evoluzione (70, 71).</p></li><li><p>La morte programmata è un caso importante di minimizzazione della funzione di perdita di livello superiore (per esempio, quella dell&rsquo;organismo) a costo di aumentare la funzione di perdita di livello inferiore (come quella delle singole cellule). Sebbene la morte cellulare programmata (strettamente controllata) sia stata originariamente scoperta negli organismi multicellulari e si sia pensato che fosse limitata a queste forme di vita complesse, il suicidio cellulare altruistico sembra ora essere un fenomeno biologico universale (71-73).
Per concludere questa sezione, che abbiamo intitolato &ldquo;fenomeni evolutivi fondamentali&rdquo;, omettendo volutamente &ldquo;biologici&rdquo;, sembra importante notare che i fenomeni da E1 a E7 sono generici e si applicano a tutti i sistemi di apprendimento, compresi quelli puramente fisici e prebiotici. Tuttavia, l&rsquo;inizio della selezione naturale (E8) segna l&rsquo;origine della vita, per cui i fenomeni da E8 a E10 appartengono al regno della biologia.</p></li></ol><h3 id=3-ottimizzazione-e-separazione-di-scala-nei-sistemi-in-evoluzione>3. Ottimizzazione e separazione di scala nei sistemi in evoluzione</h3><p>Nelle sezioni precedenti abbiamo formulato i sette principi fondamentali dell&rsquo;evoluzione da P1 a P7 e poi abbiamo sostenuto che i fenomeni evolutivi chiave da E1 a E10 possono essere interpretati e analizzati nel contesto di questi principi e, apparentemente, derivati da questi ultimi. Il passo successivo consiste nel formulare un quadro matematico che sia coerente con i principi fondamentali e che ci permetta di modellare analiticamente o numericamente i fenomeni evolutivi. Per concretezza, il quadro proposto si basa su un modello matematico di reti neurali artificiali (74, 75), ma prima delineiamo un approccio generale di ottimizzazione in una forma adatta alla modellazione dell&rsquo;evoluzione biologica.
Siamo interessati alla più ampia classe di problemi di ottimizzazione, in cui la funzione di perdita (o costo) 𝐻(𝐱,𝐪) è minimizzata rispetto ad alcune variabili addestrabili,</p><p>𝐪=(𝐪(𝑐),𝐪(𝑎), 𝐪(𝑛)),
[3.1]</p><p>per un dato insieme di variabili non addestrabili,</p><p>𝐱=(𝐱(𝑜),𝐱(𝑒)).
[3.2]</p><p>In prossimità di un minimo locale, le derivate prime della funzione di perdita media rispetto alle variabili addestrabili 𝐪 sono piccole e la profondità del minimo dipende solitamente dalle derivate seconde. In particolare, la derivata seconda può essere grande per i gradi di libertà effettivamente costanti, 𝐪(𝑐); piccola per i gradi di libertà adattabili, 𝐪(𝑎); o vicina a zero per le simmetrie o le direzioni neutre 𝐪(𝑛). La separazione delle direzioni neutre 𝐪(𝑛) in una classe speciale di variabili significa semplicemente che alcune delle variabili addestrabili possono essere cambiate senza influenzare il risultato dell&rsquo;apprendimento, cioè il valore della funzione di perdita. In altre parole, i cambiamenti neutrali sono sempre possibili. Le direzioni neutre 𝐪(𝑛) sono quelle che cambiano più velocemente tra le variabili addestrabili, perché le fluttuazioni che risultano nel loro cambiamento sono, in generale, completamente stocastiche. All&rsquo;altro estremo dello spettro di variabili, anche piccoli cambiamenti alle variabili effettivamente costanti 𝐪(𝑐) compromettono l&rsquo;intero processo di apprendimento (evoluzione): cioè, risultano in un aumento sostanziale del valore della funzione di perdita; queste variabili corrispondono a minimi profondi della funzione di perdita. Quando il bacino di attrazione di un minimo è profondo e stretto, il sistema rimane nel suo fondo per molto tempo e quindi, per descrivere tale stato, è sufficiente utilizzare informazioni discrete (cioè indicare che il sistema rimane in un determinato minimo) piuttosto che elencare tutti i valori specifici delle coordinate in uno spazio multidimensionale.</p><p>In un generico problema di ottimizzazione, la dinamica delle variabili addestrabili e non addestrabili comporta un&rsquo;ampia distribuzione di scale temporali caratteristiche 𝜏, e passare da una scala all&rsquo;altra equivale a passare da una frequenza all&rsquo;altra o, nel contesto dell&rsquo;evoluzione biologica, da un livello all&rsquo;altro di organizzazione. Per qualsiasi 𝜏 fisso, tutte le variabili possono essere suddivise in tre classi a seconda della velocità con cui cambiano rispetto alla scala temporale specificata:</p><p>variabili non addestrabili a cambiamento rapido che caratterizzano un organismo (𝐱(𝑜)) e il suo ambiente 𝐱(𝑒) e cambiano su tempi ≪𝜏;</p><p>variabili adattabili a cambiamento intermedio 𝐪(𝑎) o direzioni neutre 𝐪(𝑛) che cambiano su tempi ~𝜏; e</p><p>variabili a lento cambiamento, che sono i gradi di libertà 𝐪(𝑐) che sono già stati ben formati e sono effettivamente costanti (all&rsquo;equilibrio o quasi), cambiando solo su tempi ≫𝜏.</p><p>Come sarà evidente a breve, la separazione di queste tre classi di variabili e le interazioni tra di esse sono centrali per l&rsquo;evoluzione e la selezione a tutti i livelli di organizzazione, con il risultato di un apprendimento e una selezione pervasivi a più livelli.</p><p>A seconda della scala temporale considerata 𝜏 (o come risultato di cambiamenti ambientali), lo stesso grado di libertà dinamico può essere assegnato a diverse classi di variabili: cioè 𝐱(𝑜), 𝐱(𝑒), 𝐪(𝑐), 𝐪(𝑎) o 𝐪(𝑛). Ad esempio, sulla scala temporale più breve, che corrisponde alla vita di un singolo organismo (una generazione), le variabili adattabili sono i tratti fenotipici che rispondono rapidamente ai cambiamenti ambientali, mentre le variabili più lente e quasi costanti sono le sequenze genomiche (genotipo) che cambiano minimamente, se non del tutto. Su tempi più lunghi, corrispondenti a migliaia o milioni di generazioni, le porzioni del genoma in rapida evoluzione diventano variabili adattabili, mentre il nucleo conservato del genoma rimane nella classe quasi costante (50). Analogamente, le direzioni neutre corrispondono a cambiamenti fenotipici non consequenziali o a mutazioni genomiche neutre, a seconda della scala temporale. È noto che la stragrande maggioranza delle mutazioni è deleteria e quindi eliminata dalla selezione purificatrice o (quasi) neutra e quindi può essere persa o fissata per deriva (76, 77). Tuttavia, quando l&rsquo;ambiente cambia o sotto l&rsquo;influenza di altre mutazioni, alcune delle mutazioni neutre possono diventare benefiche [un fenomeno genetico noto come epistasi, pervasivo nell&rsquo;evoluzione (78, 79)] e, nel loro insieme, le mutazioni neutre costituiscono il serbatoio essenziale di variazione disponibile per l&rsquo;evoluzione adattativa (80). Anche quali variabili sono classificate come non addestrabili (𝐱) dipende dalla scala temporale 𝜏. Ad esempio, se un sistema di apprendimento è stato addestrato per un tempo sufficientemente lungo, alcune delle variabili addestrabili 𝐪(𝑎) o 𝐪(𝑛) potrebbero essersi già equilibrate e diventare non addestrabili.</p><h2 id=4-la-struttura-della-rete-neurale>4. La struttura della rete neurale</h2><p>Ora che abbiamo descritto un problema di ottimizzazione adatto a modellare l&rsquo;evoluzione degli organismi (o delle popolazioni di organismi), possiamo costruire una struttura matematica per risolvere tali problemi di ottimizzazione. A questo scopo, utilizziamo la teoria matematica delle reti neurali artificiali (74, 75), che è abbastanza semplice da eseguire calcoli pur essendo coerente con tutti i principi fondamentali (da P1 a P7), e quindi può essere utilizzata per modellare i fenomeni evolutivi (da E1 a E10). Ricordiamo innanzitutto un quadro generale della teoria delle reti neurali.</p><p>Consideriamo un sistema di apprendimento rappresentato come una rete neurale, con il vettore di stato descritto da variabili addestrabili 𝐪 (che descrivono una notazione collettiva per la matrice dei pesi 𝑤ˆ e il vettore bias 𝐛) e variabili non addestrabili 𝐱 (che descrivono il vettore di stato corrente dei singoli neuroni). Nel contesto biologico, le 𝐱 rappresentano collettivamente lo stato attuale dell&rsquo;organismo 𝐱(𝑜) e del suo ambiente 𝐱(𝑒), e le 𝐪 determinano il modo in cui le 𝐱 cambiano nel tempo, in particolare come l&rsquo;organismo reagisce alle sfide ambientali. Le variabili non addestrabili sono modellate come se cambiassero in passi temporali discreti</p><p>x𝑖(𝑡+1)=𝑓𝑖(∑𝑗𝑤𝑖𝑗𝑥𝑗(𝑡)+𝑏𝑖),
[4.1]</p><p>dove le 𝑓𝑖(𝑦) sono funzioni di attivazione non lineari (ad esempio, funzioni di attivazione a tangente iperbolica o a raddrizzatore). Le variabili addestrabili sono modellate come se cambiassero secondo l&rsquo;algoritmo di discesa del gradiente (o discesa stocastica del gradiente)</p><p>𝑞𝑖(𝑡+1)=𝑞𝑖(𝑡)-𝛾∂𝐻(𝐱(𝑡),𝐪(𝑡))∂𝑞𝑖,
[4.2]</p><p>dove 𝛾 è il parametro del tasso di apprendimento e 𝐻(𝐱,𝐪) è una funzione di perdita opportunamente definita (equazioni 4.3 e 4.4). In altre parole, le 𝐪 sono variabili &ldquo;lorde&rdquo; o &ldquo;principali&rdquo;, che determinano le regole della dinamica, e la dinamica di tutte le altre variabili 𝐱 è governata da queste regole, secondo l&rsquo;Eq. 4.1. Nel contesto biologico, l&rsquo;Eq. 4.1 rappresenta i cambiamenti ambientali rapidi, spesso stocastici, e la corrispondente reazione rapida degli organismi a livello di fenotipo, mentre la [4.2] riflette le dinamiche di apprendimento più lento dell&rsquo;adattamento evolutivo attraverso i cambiamenti nelle variabili intermedie adattabili: cioè la parte variabile del genoma. L&rsquo;obiettivo principale dell&rsquo;apprendimento è regolare le variabili addestrabili in modo tale da minimizzare la funzione di perdita media soggetta a condizioni limite (note anche come set di dati di addestramento), che nel nostro caso è modellato come una sequenza temporale di variabili ambientali.</p><p>Ad esempio, su una scala temporale di una singola generazione, le variabili a variazione rapida rappresentano l&rsquo;ambiente 𝐱(𝑒) e le variabili non addestrabili associate agli organismi 𝐱(𝑜), le variabili a variazione intermedia rappresentano i cambiamenti fenotipici adattivi 𝐪(𝑎) e neutri 𝐪(𝑛) e le variabili a variazione lenta 𝐪(𝑐) rappresentano il genotipo (Appendice SI, Fig. S1).</p><p>La separazione su scala temporale in biologia è evidente in tutti gli organismi. Infatti, i cambiamenti conseguenti nell&rsquo;ambiente 𝐱(𝑒) si verificano spesso su scala di millisecondi o secondi, innescando cambiamenti fisici all&rsquo;interno degli organismi 𝐱(𝑜) su scale temporali corrispondenti. In risposta, i singoli organismi rispondono con cambiamenti fenotipici sia adattivi 𝐪(𝑎) sia neutri 𝐪(𝑛) su scala di minuti o ore, sfruttando la loro plasticità fenotipica geneticamente codificata. Un esempio paradigmatico è l&rsquo;induzione di operoni batterici in risposta a un cambiamento nella composizione chimica dell&rsquo;ambiente, come il passaggio dal glucosio al galattosio come nutriente primario (81, 82). Al contrario, i cambiamenti nel genoma 𝐪(𝑐) richiedono molto più tempo. Le mutazioni si verificano in genere a tassi compresi tra 1 e 10 per ciclo di replicazione del genoma (83), che per gli organismi unicellulari equivale a una generazione che comprende da circa un&rsquo;ora a centinaia o addirittura migliaia di ore. Tuttavia, la fissazione delle mutazioni, che rappresenta un cambiamento evolutivamente stabile a livello del genoma, richiede in genere molte generazioni e quindi avviene sempre ordini di grandezza più lentamente rispetto ai cambiamenti del fenotipo. Di conseguenza, su questa scala temporale, qualsiasi cambiamento nel genoma rappresenta il terzo livello della rete, le variabili che cambiano lentamente.</p><p>Per specificare una funzione di perdita microscopica che sia appropriata per descrivere l&rsquo;evoluzione e quindi dare una forma specifica al principio fondamentale P1, notiamo innanzitutto che l&rsquo;adattamento all&rsquo;ambiente è più efficiente (cioè, il valore della funzione di perdita è più piccolo) per un sistema di apprendimento, come un organismo, che può prevedere lo stato del suo ambiente con un errore minore. Quindi, la quantità rilevante è la cosiddetta funzione di perdita &ldquo;limite&rdquo;, definita come la somma degli errori al quadrato,</p><p>𝐻𝑒(𝐱,𝐪)≡12∑𝑖∈E(𝑥(𝑒)𝑖-𝑓𝑖(𝐱(𝑜),𝐪))2,
[4.3]</p><p>dove la somma è fatta solo sulle variabili non addestrabili di contorno (o ambientali). È utile pensare alla funzione di perdita per contorno come al disallineamento tra lo stato effettivo dell&rsquo;ambiente e lo stato che sarebbe previsto dalla rete neurale se la dinamica ambientale fosse disattivata. Nelle neuroscienze, la boundary loss è strettamente legata alla sorpresa (o errore di previsione) associata alle previsioni delle sensazioni, che dipendono da un modello interno dell&rsquo;ambiente (84). Nell&rsquo;apprendimento automatico, le funzioni di boundary loss sono più spesso utilizzate nel contesto dell&rsquo;apprendimento supervisionato (35); nell&rsquo;evoluzione biologica, la &ldquo;supervisione&rdquo; proviene dall&rsquo;ambiente, che il sistema in evoluzione, come un organismo o una popolazione, sta imparando a prevedere.</p><p>Un&rsquo;altra possibilità per un sistema di apprendimento è quella di cercare il minimo della funzione di perdita &ldquo;di massa&rdquo;, definita come la somma degli errori al quadrato su tutti i neuroni:</p><p>𝐻(𝐱,𝐪)=12∑𝑖(𝑥𝑖-𝑓𝑖(𝐱(𝑜),𝐪))2.
[4.4]</p><p>La funzione di perdita di massa assume il costo aggiuntivo sostenuto per cambiare gli stati dei neuroni dell&rsquo;organismo, 𝐱(𝑜): cioè, premiare gli stati stazionari. Nel limite di un numero molto elevato di neuroni ambientali, le due funzioni di perdita sono indistinguibili, 𝐻(𝐱,𝐪)≈𝐻𝑒(𝐱,𝐪), ma la perdita di massa è più facile da gestire matematicamente (i dettagli delle funzioni di perdita al contorno e di massa sono trattati in rif. 35).</p><p>Più in generale, oltre al termine &ldquo;cinetico&rdquo; [4.4], la funzione di perdita può includere un termine &ldquo;potenziale&rdquo; 𝑉(𝐱,𝐪):</p><p>𝐻(𝐱,𝐪)=12∑𝑖(𝑥𝑖-𝑓𝑖(𝐱(𝑜),𝐪))2+𝑉(𝐱,𝐪).
[4.5]</p><p>Il termine cinetico della [4.5] riflette la capacità degli organismi 𝐱(𝑜) di prevedere i cambiamenti di stato di un dato ambiente 𝐱(𝑒) nel tempo, mentre il termine potenziale riflette la loro compatibilità con un dato ambiente e quindi la capacità di scegliere tra ambienti diversi.</p><p>Nel contesto dell&rsquo;evoluzione biologica, la fitness malthusiana 𝜑 è definita come il successo riproduttivo atteso di un dato genotipo, ovvero il tasso di variazione della prevalenza del dato genotipo in una popolazione in evoluzione (85). Tuttavia, nel contesto della teoria dell&rsquo;apprendimento, la funzione di perdita deve essere identificata con la fitness additiva: cioè,</p><p>𝐻(𝐱,𝐪)=-𝑇log𝜑(𝐱,𝐪).
[4.6]</p><p>Per una descrizione microscopica dell&rsquo;apprendimento, la costante di proporzionalità non è importante, ma come argomentiamo in dettaglio nel documento allegato (86), nella descrizione del processo evolutivo dal punto di vista della termodinamica, 𝑇 svolge il ruolo di &ldquo;temperatura evolutiva&rdquo;.
Dato un modello matematico concreto di reti neurali, ci si potrebbe chiedere se tutti i principi fondamentali dell&rsquo;evoluzione (da P1 a P7) possano essere derivati da questo modello. Tale derivazione costituirebbe un&rsquo;ulteriore prova a sostegno dell&rsquo;affermazione che l&rsquo;intero universo può essere adeguatamente descritto come una rete neurale (36). Chiaramente, l&rsquo;esistenza di una funzione di perdita (P1) segue automaticamente perché l&rsquo;apprendimento di qualsiasi rete neurale è sempre descritto rispetto a una funzione di perdita specificata (Eq. 4.4 o 4.5). Anche gli altri sei principi sembrano emergere naturalmente dalle dinamiche di apprendimento delle reti neurali. In particolare, la gerarchia di scale (P2) e i gap di frequenza (P3) sono conseguenze generiche della dinamica di apprendimento, in base alla quale un sistema che coinvolge un&rsquo;ampia gamma di variabili che cambiano a velocità diverse è attratto verso uno stato critico auto-organizzato di variabili addestrabili che cambiano lentamente (87). Ci si aspetta anche che appaiano ulteriori divari tra i livelli di organizzazione attraverso transizioni di fase, come risulta evidente nella descrizione termodinamica dell&rsquo;evoluzione che abbiamo sviluppato nel documento di accompagnamento (86). La rinormalizzabilità (P4) è una conseguenza diretta della seconda legge dell&rsquo;apprendimento (35), secondo la quale l&rsquo;entropia di un sistema (e di conseguenza la complessità della rete neurale o il rango della sua matrice di pesi) diminuisce con l&rsquo;apprendimento. Questo fenomeno è stato osservato nelle simulazioni di reti neurali (35) ed è l&rsquo;esatto tipo di dinamica che può rendere il sistema rinormalizzabile anche se è partito come una rete neurale altamente entangled (grande rango della matrice dei pesi) e non rinormalizzabile. I principi di estensione (P5) e replica (P6) indicano semplicemente che variabili aggiuntive possono portare a un aumento o a una diminuzione del valore della funzione di perdita (35). È inoltre importante notare che nelle reti neurali è possibile ottenere un ulteriore vantaggio computazionale (&ldquo;vantaggio quantico&rdquo;) se il numero di IPU può variare (88). Pertanto, per ottenere tale vantaggio, un sistema deve imparare a replicare ed eliminare le sue UIP (P6). Infine, nel Dogma centrale generalizzato della biologia molecolare, illustriamo come la trasformata di Fourier (o, più in generale, la trasformata wavelet) dei gradi di libertà dell&rsquo;ambiente possa essere utilizzata per imparare l&rsquo;ambiente e come la trasformata inversa possa essere utilizzata per prevederlo. Quindi, per essere in grado di prevedere l&rsquo;ambiente (e quindi per essere competitivo), qualsiasi sistema in evoluzione deve apprendere il meccanismo alla base di questo flusso di informazioni asimmetriche (P7).</p><h2 id=5-apprendimento-multilivello>5. Apprendimento multilivello</h2><p>Nelle sezioni precedenti abbiamo sostenuto che il processo di apprendimento divide naturalmente tutte le variabili dinamiche in tre classi distinte: quelle a variazione rapida, 𝐱(𝑜) e 𝐱(𝑒); quelle a variazione intermedia, 𝐪(𝑎) e 𝐪(𝑛) (𝐪(𝑛) è più veloce di 𝐪(𝑎)); e quelle a variazione lenta, 𝐪(𝑐) (Appendice SI, Fig. S1). Evidentemente, questa separazione delle variabili dipende dalla scala temporale 𝜏 durante la quale il sistema viene osservato, e le variabili migrano tra le classi quando 𝜏 aumenta o diminuisce (Appendice SI, Fig. S2). Più lungo è il tempo, più variabili raggiungono l&rsquo;equilibrio e quindi possono essere modellate come non addestrabili e a rapida variazione, 𝐱(𝑒), e meno variabili rimangono a lenta variazione e possono essere modellate come effettivamente costanti 𝐪(𝑐). In altre parole, molte variabili che sono quasi costanti su scale temporali brevi migrano verso la classe intermedia su scale temporali più lunghe, mentre le variabili della classe intermedia migrano verso la classe veloce.</p><p>In termini biologici, se consideriamo le dinamiche di apprendimento sulla scala temporale di una generazione, allora 𝐪(𝑎) e 𝐪(𝑛) rappresentano le variabili fenotipiche e 𝐪(𝑐) le variabili genotipiche; tuttavia, su scale temporali molto più lunghe di più generazioni, le dinamiche di apprendimento di popolazioni (o comunità) di organismi diventano rilevanti. Su tali scale temporali, le variabili genotipiche acquisiscono una dinamica, con l&rsquo;entrata in gioco della selezione purificatrice e positiva, mentre le variabili fenotipiche si equilibrano progressivamente. Esiste una chiara connessione tra le dinamiche di apprendimento, comprese quelle dei sistemi biologici, e la rinormalizzabilità delle teorie fisiche (P4). Infatti, dal punto di vista di un algoritmo di apprendimento efficiente, i parametri che controllano le dinamiche di apprendimento, come il tasso di apprendimento effettivo (o di elaborazione dell&rsquo;informazione) 𝛾, possono variare da una scala temporale all&rsquo;altra (per esempio, da singoli organismi a popolazioni o comunità di organismi), ma i principi generali e le dipendenze specifiche catturate nelle equazioni di cui sopra che governano le dinamiche di apprendimento su scale temporali diverse rimangono gli stessi. Questa universalità del processo di apprendimento su diverse scale temporali e la suddivisione delle variabili in classi temporali sono definite apprendimento multilivello.</p><p>Più precisamente, l&rsquo;apprendimento multilivello è una proprietà dei sistemi di apprendimento che permette alle equazioni di base dell&rsquo;apprendimento, come la [4.4], di rimanere le stesse a tutti i livelli di organizzazione, ma ai parametri che descrivono la dinamica, come 𝛾(𝜏), di dipendere dal livello o dalla scala temporale 𝜏. Ad esempio, se il tasso effettivo di apprendimento (o di elaborazione dell&rsquo;informazione) 𝛾(𝜏) diminuisce con la scala temporale 𝜏, allora il tempo di elaborazione locale, che dipende da 𝛾(𝜏), funziona in modo diverso per le diverse variabili addestrabili: più lento per le variabili a cambiamento lento (o 𝜏 più grande) e più veloce per quelle a cambiamento rapido (o 𝜏 più piccolo). Per un sistema di questo tipo, il concetto di tempo globale (cioè lo stesso tempo per tutte le variabili) diventa irrilevante e va sostituito con il tempo proprio o locale, che viene definito per ogni scala 𝜏 separatamente:</p><p>𝑡𝜏∝𝛾(𝜏)𝑡.
[5.1]</p><p>Questo effetto assomiglia molto ai fenomeni di dilatazione del tempo in fisica, con la differenza che nella relatività speciale e generale la dilatazione del tempo è legata alla possibilità di movimento tra orologi (o variabili) lenti e veloci (89). Per illustrare il ruolo della dilatazione temporale in biologia, si considerino solo due tipi di variabili: quelle che cambiano lentamente e quelle che cambiano velocemente. Le variabili lente dovrebbero essere in grado di &ldquo;esternalizzare&rdquo; alcuni compiti computazionali alle variabili veloci. Poiché l&rsquo;orologio locale delle variabili a variazione rapida scorre più velocemente, le variabili a variazione lenta possono sfruttare quelle a variazione rapida per accelerare il calcolo, il che verrebbe premiato dall&rsquo;evoluzione. Il flusso di informazioni tra le variabili a variazione lenta e quelle a variazione rapida nella direzione opposta è altrettanto vantaggioso, perché le variabili a variazione rapida possono utilizzare quelle a variazione lenta per immagazzinare informazioni utili da recuperare in futuro: in altre parole, le variabili lente funzionano come memoria a lungo termine. Nella prossima sezione mostreremo che questa cooperazione tra variabili a cambiamento lento e veloce, che è una manifestazione concreta del principio P7, corrisponde a un fenomeno biologico cruciale come il dogma centrale della biologia molecolare (60).</p><h2 id=6-dogma-centrale-generalizzato-della-biologia-molecolare>6. Dogma centrale generalizzato della biologia molecolare</h2><p>In termini di teoria dell&rsquo;apprendimento, le due direzioni del flusso asimmetrico di informazioni (P7) rappresentano l&rsquo;apprendimento dello stato dell&rsquo;ambiente e la previsione dello stato dell&rsquo;ambiente dai risultati dell&rsquo;apprendimento. Per l&rsquo;apprendimento, le informazioni passano dalle variabili più veloci a quelle più lente, mentre per la previsione, le informazioni fluiscono nella direzione opposta, dalle variabili più lente a quelle più veloci. Un&rsquo;analisi più formale dei flussi di informazione asimmetrici (o un Dogma Centrale generalizzato) può essere effettuata mediante la propagazione in avanti (dalle variabili lente a quelle veloci) e la propagazione all&rsquo;indietro (dalle variabili veloci a quelle lente) delle informazioni nell&rsquo;ambito del modello matematico delle reti neurali sviluppato nelle sezioni precedenti (Appendice SI, Fig. S3).
Consideriamo variabili ambientali non addestrabili che cambiano continuamente con il tempo 𝐱(𝑒)(𝑡), mentre l&rsquo;obiettivo di apprendimento di un organismo è quello di prevedere 𝐱(𝑒)(𝑡) al tempo 𝑡>𝜏 dato che è stato osservato per il tempo 0&lt;𝑡&lt;𝜏. Pertanto, l&rsquo;organismo deve estrapolare la funzione 𝐱(𝑒)(𝑡) per 𝑡>𝜏, e per farlo deve essere in grado di memorizzare e recuperare i valori dei coefficienti di Fourier</p><p>𝐪𝑘=1𝜏∫𝜏0𝑑𝑡𝐱(𝑒)(𝑡)𝑒-𝑖2𝜋𝑓𝑘𝑡,
[6.1]</p><p>o, più in generale, i coefficienti wavelet</p><p>𝐪𝑘=1𝜏∫𝜏0𝑑𝑡𝑊𝑘(𝜏-𝑡)𝐱(𝑒)(𝑡)𝑒-𝑖2𝜋𝑓𝑘𝑡,
[6.2]</p><p>per funzioni finestra opportunamente definite 𝑊𝑖. Quindi, si potrebbe fare una previsione estrapolando 𝐱(𝑒)(𝑡) utilizzando la trasformazione inversa</p><p>𝐱(𝑒)(𝑡+𝛿)≈2∑𝑘=𝑘𝑚𝑖𝑛𝑘𝑚𝑎𝑥Re(𝐪𝑘𝑒𝑖2𝜋𝑓𝑘𝛿),
[6.3]</p><p>per qualche 𝛿>0, che non è troppo grande rispetto a 𝜏. Tuttavia, in generale, il numero totale di coefficienti (di Fourier o wavelet) 𝐪𝑘 sarebbe contabilmente infinito. Pertanto, qualsiasi organismo di dimensioni finite deve &ldquo;decidere&rdquo; quali frequenze osservare (e ricordare) e quali filtrare (e &ldquo;dimenticare&rdquo;).
Supponiamo che l&rsquo;organismo &ldquo;decida&rdquo; di osservare/ricordare solo le frequenze discrete</p><p>𝑓𝑚𝑖𝑛≡𝑓𝑘𝑚𝑖𝑛,&mldr;,𝑓𝑘𝑚𝑎𝑥≡𝑓𝑚𝑎𝑥,
[6.4]</p><p>e dimenticare tutto il resto. Quindi, per prevedere lo stato dell&rsquo;ambiente [6.3] e, di conseguenza, minimizzare la funzione di perdita [4.5], l&rsquo;organismo deve essere in grado di memorizzare, recuperare e regolare le informazioni sui coefficienti 𝐪𝑘 in alcune variabili addestrabili 𝐪(𝑎).
Dato questo semplice modello, possiamo studiare il flusso di informazioni tra diverse variabili non addestrabili dell&rsquo;organismo 𝐱(𝑜). A tal fine, è conveniente organizzare le variabili come</p><p>𝐱=(𝐱(𝑜),𝐱(𝑒))=(𝐱𝑘𝑚𝑖𝑛,&mldr;,𝐱𝑘𝑚𝑎𝑥,𝐱(𝑒)),
[6.5]</p><p>dove</p><p>𝐱𝑘(𝑡+𝛿)≈2∑𝑙=𝑘𝑚𝑖𝑛𝑘Re(𝐪𝑙𝑒𝑖2𝜋𝑓𝑙𝛿),
[6.6]</p><p>e assumiamo che le informazioni rilevanti sugli 𝐪𝑖 siano memorizzate nelle variabili addestrabili adattabili</p><p>𝐪(𝑎)=(𝐪𝑘𝑚𝑖𝑛,&mldr;,𝐪𝑘𝑚𝑎𝑥).
[6.7]</p><p>Nella stima di 𝐱𝑘(𝑡+𝛿) in Eq. 6.6, si assume che tutti i modi a frequenza più alta abbiano una media pari a zero, come spesso accade se si è interessati solo alla scala temporale 𝑓-1𝑘. Una stima migliore può essere ottenuta utilizzando, ancora una volta, le idee del flusso del gruppo di rinormalizzazione secondo il principio fondamentale P4. Per rendere efficiente l&rsquo;apprendimento (e quindi la sopravvivenza), è fondamentale troncare l&rsquo;insieme delle variabili rilevanti per l&rsquo;apprendimento. Il punto principale è che i modi a più alta frequenza possono ancora contribuire statisticamente e quindi una stima migliore di 𝐱𝑘(𝑡+𝛿) si otterrebbe modificando opportunamente i valori dei coefficienti 𝐪𝑘. In ogni caso, per fare una previsione vera e propria, l&rsquo;organismo dovrebbe prima calcolare 𝐱𝑘(𝑡+𝛿) per piccoli 𝑓𝑘 e poi passare il risultato al livello successivo per calcolare 𝐱𝑘+1(𝑡+𝛿) per 𝑓𝑘+1 e così via. Tali calcoli possono essere descritti da una semplice mappatura</p><p>𝐱𝑘+1(𝑡+𝛿)=𝐱𝑘(𝑡+𝛿)+2Re(𝐪𝑘+1𝑒𝑖2𝜋𝑓𝑘+1𝛿),
[6.8]</p><p>che può essere interpretata come il passaggio di dati da uno strato all&rsquo;altro in una rete neurale profonda multistrato (Appendice SI, Fig. S2). L&rsquo;equazione 6.8 implica che, durante la fase di previsione, le informazioni rilevanti fluiscono solo dalle variabili che codificano le basse frequenze alle variabili che codificano le alte frequenze, ma non nella direzione inversa. In altre parole, nel processo di previsione dell&rsquo;ambiente, l&rsquo;informazione si propaga dalle variabili più lente a quelle più veloci: cioè, dal genotipo al fenotipo o dagli acidi nucleici alle proteine (da qui, il Dogma Centrale). Poiché in questo processo cambiano solo le variabili veloci, la previsione dello stato dell&rsquo;ambiente è rapida, come del resto è necessario che sia per la sopravvivenza dell&rsquo;organismo. Al contrario, nel processo di apprendimento dell&rsquo;ambiente, l&rsquo;informazione si propaga in senso opposto, cioè dalle variabili più veloci a quelle più lente. Tuttavia, questa retropropagazione non è una microscopica inversione della propagazione in avanti, ma un processo distinto e molto più lento (dato che sono necessari cambiamenti nelle variabili lente) che coinvolge la mutazione e la selezione.</p><p>Pertanto, il significato del dogma centrale generalizzato dal punto di vista della teoria dell&rsquo;apprendimento - e della nostra teoria dell&rsquo;evoluzione - è che le dinamiche lente (cioè l&rsquo;evoluzione su una lunga scala temporale) dovrebbero essere per lo più indipendenti dalle variabili veloci. In termini meno formali, le variabili lente determinano le regole del gioco, e cambiare queste regole in base ai risultati di alcuni giochi particolari sarebbe dannoso per l&rsquo;organismo. L&rsquo;ottimizzazione all&rsquo;interno dello spazio di opportunità vincolato da regole temporalmente stabili è vantaggiosa rispetto all&rsquo;ottimizzazione senza tali vincoli. Il compromesso tra ottimizzazione globale e locale è una proprietà generale e intrinseca dei sistemi frustrati (E2). Affinché il sistema funzioni in modo efficiente, l&rsquo;impatto dell&rsquo;ottimizzazione locale sull&rsquo;ottimizzazione globale deve essere limitato. La separazione delle forme di memoria a lungo e a breve termine attraverso basi elementari diverse (acidi nucleici e proteine) risponde a questo obiettivo.</p><h2 id=7-discussione>7. Discussione</h2><p>In questo lavoro abbiamo delineato una teoria dell&rsquo;evoluzione sulla base della teoria dell&rsquo;apprendimento. Il parallelo tra l&rsquo;apprendimento e l&rsquo;evoluzione biologica diventa evidente non appena si stabilisce la mappatura tra la funzione di perdita e la funzione di fitness (Eq. 4.6). In effetti, entrambi i processi rappresentano il movimento di un sistema in evoluzione (che apprende) su un paesaggio di fitness (funzione di perdita), dove gli spostamenti adattativi (di apprendimento) verso l&rsquo;alto sono i più conseguenti, anche se gli spostamenti neutri sono più comuni e occasionalmente si verificano anche spostamenti verso il basso. Tuttavia, andiamo oltre l&rsquo;ovvia analogia e tracciamo una corrispondenza dettagliata tra le caratteristiche essenziali dei processi evolutivi e di apprendimento. Probabilmente, la più importante comunanza fondamentale tra evoluzione e apprendimento è la stratificazione delle variabili addestrabili (gradi di libertà) in classi che differiscono per il tasso di cambiamento. Almeno negli ambienti complessi, tutto l&rsquo;apprendimento è multilivello, e così tutta la selezione che è rilevante per il processo evolutivo. Il quadro dell&rsquo;evoluzione come apprendimento qui sviluppato implica che l&rsquo;evoluzione della complessità biologica sarebbe impossibile senza che la MLS permei l&rsquo;intera storia della vita. In questa prospettiva, l&rsquo;emergere di nuovi livelli di organizzazione, nell&rsquo;apprendimento e nell&rsquo;evoluzione, e in particolare l&rsquo;MTE rappresentano vere e proprie transizioni di fase, come suggerito in precedenza (41). Tali transizioni possono essere analizzate in modo coerente solo nel limite termodinamico, che viene affrontato in dettaglio nel documento allegato (86).</p><p>L&rsquo;origine della complessità e della memoria a lungo termine da semplici leggi fisiche fondamentali è uno dei problemi più difficili di tutta la scienza. Un approccio molto diffuso è quello della sinergetica, inaugurato da Haken (91, 92), e della relativa termodinamica non di equilibrio, fondata da Prigogine e Stengers (93), che utilizzano gli strumenti matematici della teoria dei sistemi dinamici, come la teoria delle biforcazioni e l&rsquo;analisi degli attrattori. Tuttavia, questi concetti appaiono troppo generici e semplificati per analizzare utilmente i fenomeni biologici, che sono molto più complessi delle &ldquo;strutture dissipative&rdquo; che sono al centro della termodinamica non di equilibrio, come, ad esempio, le reazioni chimiche ad autoonde.</p><p>Un&rsquo;alternativa è l&rsquo;approccio basato sulla teoria dei vetri di spin (43, 94), che impiega l&rsquo;apparato matematico della fisica statistica e sembra fornire una visione più profonda dell&rsquo;origine della complessità. Tuttavia, il paesaggio energetico dei vetri di spin contiene troppi minimi che sono troppo superficiali per rendere conto della memoria a lungo termine che è centrale nella biologia (12, 41). Pertanto, è probabile che sia necessaria una generalizzazione del concetto di spin glass per un&rsquo;applicazione produttiva nella biologia evolutiva (95).</p><p>Un approccio popolare e promettente è la criticità auto-organizzata (SOC), un concetto sviluppato da Bak et al. (96, 97). Sebbene sia rilevante in contesti biologici (12), la SOC, per definizione, implica l&rsquo;auto-similarità tra diversi livelli di organizzazione, mentre la complessità biologicamente rilevante è piuttosto associata a fenomeni emergenti distinti su diverse scale spazio-temporali (90).</p><p>Un difetto fondamentale di tutti questi approcci è che non includono, almeno non come componente principale, concetti evolutivi, come la selezione naturale. Il quadro della teoria dell&rsquo;apprendimento qui utilizzato ci permette di unificare naturalmente le descrizioni dei fenomeni fisici e biologici in termini di ottimizzazione per tentativi ed errori e di funzioni di perdita (fitness). In effetti, un punto chiave della presente analisi è che la maggior parte dei nostri principi generali si applica sia ai sistemi viventi che a quelli non viventi.</p><p>La corrispondenza dettagliata tra le caratteristiche chiave dei processi di apprendimento e di evoluzione biologica implica che non si tratta di una semplice analogia, ma piuttosto di un riflesso della profonda unità dei processi evolutivi che si verificano nell&rsquo;universo. Infatti, la separazione dei gradi di libertà rilevanti in più classi temporali è onnipresente nell&rsquo;universo, dalle particelle subatomiche composite, come i protoni, agli atomi, alle molecole, alle forme di vita, ai sistemi planetari e agli ammassi di galassie. Se l&rsquo;intero universo viene concettualizzato come una rete neurale (36), tutti questi sistemi possono essere considerati emergenti dalla dinamica di apprendimento. Inoltre, la separazione di scala e la rinormalizzabilità sembrano essere condizioni essenziali perché un universo sia osservabile. Secondo la teoria dell&rsquo;evoluzione qui delineata, qualsiasi universo osservabile è costituito da sistemi che subiscono l&rsquo;apprendimento o, sinonimicamente, l&rsquo;evoluzione adattativa e, di fatto, l&rsquo;universo stesso è un sistema di questo tipo (36). Il famoso detto di Dobzhansky (98), quindi, può e probabilmente dovrebbe essere riformulato come &ldquo;nulla al mondo è comprensibile se non alla luce dell&rsquo;apprendimento&rdquo;.</p><p>All&rsquo;interno della teoria dell&rsquo;evoluzione qui delineata, la differenza tra sistemi viventi e non viventi, per quanto importante, può essere considerata come una differenza nel tipo e nel grado di ottimizzazione, in modo che tutti i fenomeni evolutivi possano essere descritti all&rsquo;interno dello stesso quadro formale della teoria dell&rsquo;apprendimento. In particolare, qualsiasi problema di ottimizzazione complesso può essere affrontato solo con un algoritmo di apprendimento stocastico: da qui l&rsquo;ubiquità della selezione. L&rsquo;origine della vita può quindi essere concettualizzata nel quadro dell&rsquo;apprendimento multilivello, come mostriamo esplicitamente nel documento allegato (86). Il momento in cui inizia la vita può essere naturalmente associato all&rsquo;emergere di una classe distinta di variabili che cambiano lentamente, che sono digitalizzate e quindi possono essere replicate con precisione; queste variabili digitali immagazzinano e forniscono informazioni per la propagazione in avanti per prevedere lo stato dell&rsquo;ambiente. In termini biologici, questo punto focale corrisponde all&rsquo;avvento dei replicatori (genomi) che trasportano informazioni sul funzionamento dei riproduttori all&rsquo;interno dei quali risiedono (99). Questo è anche il momento in cui scatta la selezione naturale (darwiniana) (64). La nostra teoria dell&rsquo;evoluzione implica che questa fase cruciale sia stata preceduta dall&rsquo;evoluzione della &ldquo;pre-vita&rdquo;, che comprendeva riproduttori privi di genomi, ma che tuttavia erano sistemi di apprendimento soggetti alla selezione per la persistenza. Le micelle autoriproducenti che ospitano reti di reazioni protometaboliche autocatalitiche sembrano essere modelli plausibili di tali riproduttori primordiali (100). I primi replicatori (molecole di RNA) si sarebbero evoluti all&rsquo;interno di questi riproduttori, forse, inizialmente, come parassiti molecolari (E9), ma successivamente, sotto selezione per la capacità di immagazzinare, esprimere e condividere informazioni essenziali per l&rsquo;intero sistema. Questo passaggio chiave ha aumentato notevolmente l&rsquo;efficienza dell&rsquo;evoluzione/apprendimento e ha fornito una memoria a lungo termine che è persistita per tutta la storia della vita, consentendo l&rsquo;inizio della selezione naturale e la diversificazione senza precedenti delle forme di vita (E5). Va sottolineato che, rispetto ai modelli evolutivi esistenti che esplorano le dinamiche dei replicatori, l&rsquo;approccio all&rsquo;apprendimento qui descritto è più microscopico, in quanto l&rsquo;esistenza dei replicatori non è inizialmente ipotizzata, ma appare piuttosto come una proprietà emergente delle dinamiche di apprendimento multilivello. Affinché l&rsquo;apprendimento sia efficiente, è essenziale la capacità del sistema di aggiungere nuove variabili adattabili. In termini biologici, ciò implica l&rsquo;espandibilità del genoma (cioè la capacità di aggiungere nuovi geni), il che ha reso necessaria la transizione dall&rsquo;RNA al DNA come substrato del genoma, dati gli evidenti vincoli intrinseci di dimensione delle molecole di RNA che si replicano. Un&rsquo;altra condizione essenziale per un apprendimento efficiente è la condivisione delle informazioni, che nel contesto biologico corrisponde al trasferimento genico orizzontale. L&rsquo;essenzialità del trasferimento genico orizzontale nelle prime fasi dell&rsquo;evoluzione della vita è percepita come la causa dell&rsquo;universalità del macchinario di traduzione e del codice genetico in tutte le forme di vita conosciute (101). Il modello concettuale dell&rsquo;origine della vita implicito nel nostro quadro teorico basato sull&rsquo;apprendimento sembra essere pienamente compatibile con il chemiotonio di Gánti, un modello di emergenza ed evoluzione delle protocellule basato su reti di reazioni autocatalitiche (102-104).</p><p>Lo scenario dell&rsquo;origine della vita nell&rsquo;ambito dell&rsquo;attuale teoria dell&rsquo;evoluzione, anche se formulato in termini più generali, implica che l&rsquo;emergere della complessità commisurata alla vita è una tendenza generale nell&rsquo;evoluzione dei sistemi complessi. A prima vista, questa conclusione potrebbe sembrare in contrasto con l&rsquo;entità della complessificazione implicata nell&rsquo;origine della vita [basti pensare alla complessità del sistema di traduzione (7)] e con l&rsquo;unicità di questo evento, almeno sulla Terra e probabilmente su una scala cosmica molto più ampia. Tuttavia, l&rsquo;origine della vita sembra essere un risultato atteso dell&rsquo;apprendimento soggetto a vincoli rilevanti, come la presenza delle sostanze chimiche necessarie in concentrazioni sufficienti. Tali vincoli renderebbero la vita un fenomeno raro, ma probabilmente tutt&rsquo;altro che unico sulla scala dell&rsquo;universo. A volte si sostiene che l&rsquo;universo sia sintonizzato per l&rsquo;esistenza della vita (105). In questa sede sosteniamo che l&rsquo;universo è auto-sintonizzato per la comparsa della vita.</p><p>Ovviamente, l&rsquo;analisi presentata qui e nel documento di accompagnamento (86) è solo un abbozzo di teoria dell&rsquo;evoluzione come apprendimento. I dettagli e le implicazioni, comprese quelle direttamente testabili, restano da definire.</p><h2 id=cita-il-paper>Cita il paper</h2><p>Vanchurin, Vitaly, Yuri I. Wolf, Mikhail I. Katsnelson, and Eugene V. Koonin. 2022. “Toward a Theory of Evolution as Multilevel Learning.” <em>Proceedings of the National Academy of Sciences of the United States of America</em> 119 (6). <a href=https://doi.org/10.1073/pnas.2120037119>https://doi.org/10.1073/pnas.2120037119</a></p><ul class=pa0></ul><div class="mt6 instapaper_ignoref"></div></div><aside class="w-30-l mt6-l"></aside></article></main><footer class="bg-blue bottom-0 w-100 pa3" role=contentinfo><div class="flex justify-between"><a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href=https://biologicinfo.github.io/rjevolution>&copy; Appunti di Etica ed Evoluzione del Ringiovanimento 2022</a><div><div class=ananke-socials></div></div></div></footer></body></html>